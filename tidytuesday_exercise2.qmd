---
title: "Tidy Tuesday Exercise2"
output: 
  html_document:
    toc: FALSE
---

# Getting Started

#### Libraries

```{r}
library(tidyverse)
library(here)
library(rsample) #Data spliting
library(tidymodels)
tidymodels_prefer(quiet =FALSE) #Removes package masking conflicts 

#ANOVA
library(ggpubr)
library(broom)
library(AICcmodavg)

#Decision Trees
#library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(caret)
library(pROC)
library(tree)

#Naive
library(naivebayes)
library(psych)
```

#### Load Data

```{r}
cage<- read_csv(here("data", "cage-free-percentages.csv")) #Cage free %
egg<- read_csv(here("data", "egg-production.csv"))
```

#### Look at Data sets

```{r}
head(cage)
head(egg)
```

Note: Within the `egg` data set, the value 'all' includes cage-free and conventional housing.

# Initial Cleaning

#### Cage Data set

```{r}
cage<- 
  cage %>%
  select(!source) #Removes Source column
```

#### Egg Data set

```{r}
egg<- 
  egg %>%
  select(!source) %>% #Removes Source Column
  mutate(egg_hen = n_eggs / n_hens, #Creates column that gives an average eggs per hen
         prod_process = recode(prod_process, "cage-free (non-organic)" = "cage_free_no"),
         prod_process = recode(prod_process, "cage-free (organic)" = "cage_free_o"))


```

# Exploratory Visualization

#### Fig. 1: Percentage of cage free gens and eggs

```{r}
cage %>%
  ggplot() + geom_line( #Percent of cage free hens (orange)
    aes(x = observed_month,
        y = percent_hens),
    color = "#f68f3c") +
  geom_point(
    aes(x = observed_month,
        y = percent_hens),
    size = 1,
    color = "#f68f3c")+
  geom_line( #Percent of cage free eggs (green)
    aes(x = observed_month,
        y = percent_eggs),
    color = "#5e8d5a",
    size = 1) +
  theme_bw() +
  labs(x = "Year",
       y = "%",
       title = "Fig.1 :Percentage of Cage Free Hens and Eggs over Time") 
  
```

Percentage of Hens are in orange, and percentage of eggs are in green. We see an increase in percentage of cage free eggs and hens over time.

#### Fig. 2: Eggs per Hen across production type and process

```{r}
egg %>%
  ggplot() + geom_boxplot(
    aes(x = prod_type,
        y = egg_hen,
        color = prod_process)) +
  theme_bw()+
  labs(title = "Fig. 2: Eggs per Hen Across Production Process and Type",
       x = "Production Type",
       y = "Eggs per Hen",
       color = "Production Process") + 
  scale_color_manual(values = c(all = "#ee6f68",
                                cage_free_no = "#5e8d5a",
                                cage_free_o = "#f68f3c")) 
```

Figure 2 depicts a stark difference in the number eggs per hen between hatching eggs and table eggs. No difference is observed between the production processes within the table eggs.

#### Fig. 3: Eggs per hen over time

```{r}
egg %>%
  ggplot() + geom_line(
    aes(x = observed_month,
        y = egg_hen,
        color = prod_type)) +
  theme_bw()+
  labs(x = "Year",
       y = "Eggs per Hen",
       title = "Fig. 3: Eggs Per hen over Time by Production Type",
       color = "Production Type")
```

Similarly, Figure 3 shows that more eggs per hen are produced from the table eggs process.

#### Fig.4 Eggs x Hens

```{r}
p<- egg %>%
  filter(prod_type %in% "table eggs",
         !prod_process %in% "all") 
p %>%
  ggplot() + geom_point(
    aes(x = n_hens,
        y = n_eggs,
    color = prod_process)) +
  theme_bw()+
  labs(x = "#Eggs",
       y = "#Hens",
       title = "Number of eggs x Number of Hens") 
```

This is a very linear correlation- but not a very interesting question.

#### Question/Hypothesis:

Because a higher portion of the population consumes eggs as opposed to hatching chicks, and due to the energetically taxing/limiting process of hatching, we expect to see a significantly higher effort towards the production of table eggs as opposed to hatching eggs.

-   Predictor: Production Type (Hatching or Table Eggs)

-   Outcome: Eggs per hen

# Final Cleaning

We will use only the egg dataset.

#### Remove Columns that are not needed for analysis

```{r}
egg <- 
  egg %>%
  select(prod_type, egg_hen)
```

#### Split into Test and Train Set

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(222)
# Put 3/4 of the data into the training set 
data_split <- initial_split(egg, prop = 3/4)

# Create data frames for the two sets:
train <- training(data_split)
test  <- testing(data_split)
```

# Modeling

## 0. NULL MODEL

#### 5-Fold Cross Validation

```{r}
fold_egg_train <- vfold_cv(train, v = 5, repeats = 5, strata = egg_hen)

fold_egg_test <- vfold_cv(test, v = 5, repeats = 5, strata = egg_hen)
```

#### Recipes

```{r}
#Train Data
egg_rec_train <- 
  recipe(egg_hen ~ prod_type, data = train)# %>%
 # step_dummy(all_nominal(), -all_outcomes()) %>% 
 # step_ordinalscore() %>%
 # step_zv(all_predictors()) 


#Test Data
egg_rec_test <- 
 recipe(egg_hen ~ prod_type, data = test) #%>%
 # step_dummy(all_nominal(), -all_outcomes()) %>%
#  step_ordinalscore() %>%
 # step_zv(all_predictors()) 

```

#### Define Model

```{r}
lm_mod <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

```

#### Train Data

##### Recipe

```{r}
null_train_rec<- 
recipe(egg_hen ~ 1, data = train)

null_train_rec
```

##### Workflow

```{r}
null_train_wf <- 
  workflow() %>% 
  add_model(lm_mod) %>% 
  add_recipe(null_train_rec)
```

##### Fit

```{r, results=FALSE}
null_train_fit <- 
  fit_resamples(null_train_wf, resamples = fold_egg_train)
```

##### RMSE Metric

```{r}
null_train_met <- collect_metrics(null_train_fit)

null_train_met
```

**RMSE = 2.18 and STD_ERR = 0.011**

#### Test Data

##### Recipe

```{r}
null_test_rec<- 
  recipe(egg_hen ~ 1, data = test)
```

##### Workflow

```{r}
null_test_wf <- 
  workflow() %>% 
  add_model(lm_mod) %>% 
  add_recipe(null_test_rec)
```

##### Fit

```{r}
null_test_fit <- 
  fit_resamples(null_test_wf, resamples = fold_egg_test)
```

##### Metrics: RMSE

```{r}
null_test_met <- collect_metrics(null_test_fit)

null_test_met
```

**RMSE = 2.19 and STD_ERR = 0.084\
**

## 1. LINEAR MODEL

#### Run Model

```{r}
#Set up linear model
lm_mod<- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

#Workflow that adds recipe to model
lm_wflow<- 
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(egg_rec_train)

#Use workflow to fit model to  data set
egg_fit<- lm_wflow %>%
  fit(data = train)

#View as Tibble 
egg_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

#### Assess Performance

##### RMSE

```{r}
aug_test <- augment(egg_fit, train)
rmse <- aug_test %>% rmse(truth = egg_hen, .pred)
rsq <- aug_test %>% rsq(truth = egg_hen, .pred)
metrics<- full_join(rmse, rsq)
metrics
```

**RMSE = 0.87**

##### Residuals

```{r}
egg_mod<- lm(egg_hen ~ prod_type, data = train)
res<- resid(egg_mod)
plot(fitted(egg_mod), res)
abline(0,0)
```

<!--# Need a 4th  -->

## 2. ANOVA MODEL

**ANOVA** is a [statistical test](https://www.scribbr.com/statistics/statistical-tests/) for estimating how a quantitative [dependent variable](https://www.scribbr.com/methodology/independent-and-dependent-variables/#dependent) changes according to the levels of one or more categorical [independent variables](https://www.scribbr.com/methodology/independent-and-dependent-variables/#independent). ANOVA tests whether there is a difference in means of the groups at each level of the independent variable.

#### Null

```{r}
#Model
anova_n<- 
  aov(egg_hen ~ 1, data = egg)
summary(anova_n)

#Assess Performance
RSS<- c(crossprod(anova_n$residuals))
MSE<- RSS / length(anova_n$residuals)
RMSE <- sqrt(MSE)
RMSE
```

**RMSE of Null = 2.18**

#### Actual Model

```{r}
#Model
anova<- 
  aov(egg_hen ~ prod_type, data = train)
summary(anova)

#Assess Performance
RSS<- c(crossprod(anova$residuals))
MSE<- RSS / length(anova$residuals)
RMSE <- sqrt(MSE)
RMSE
```

**RMSE for Actual Model = 0.91**

#### Plot

```{r}
plot(anova, 1)

```

## 3. TREE MODEL

#### Model Specification

```{r}
tune_spec_dtree <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()) %>%
  set_engine("rpart") %>% 
  set_mode("regression")

tune_spec_dtree
```

Think of `tune()` here as a placeholder. After the tuning process, we will select a single numeric value for each of these hyperparameters. For now, we specify our parsnip model object and identify the hyperparameters we will `tune()`.

#### Workflow Definition

```{r}
dtree_wf <- workflow() %>%
  add_model(tune_spec_dtree) %>%
  add_recipe(egg_rec_train)
```

#### Tuning Grid Specification

We can create a regular grid of values to try using some convenience functions for each hyperparameter:

```{r}
#create a regular grid of values for using convenience functions for each hyperparameter.
tree_grid_dtree <-
  grid_regular(
    cost_complexity(), 
    tree_depth(), 
    levels = 5)

tree_grid_dtree
```

#### Tuning using Cross-validation and `tune_grid()` function

```{r}
dtree_resample <- 
  dtree_wf %>% 
  tune_grid(
    resamples = fold_egg_train,
    grid = tree_grid_dtree)
    
```

Once we have our tuning results, we can both explore them through visualization and then select the best result. The function `collect_metrics()` gives us a tidy tibble with all the results

```{r}
dtree_resample %>%
  collect_metrics()
```

##### Plot Model using autoplot()

```{r}
dtree_resample %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

<!--# SHould it look like this? -->

\

#### Metrics: RMSE

The [`show_best()`](https://tune.tidymodels.org/reference/show_best.html) function shows us the top 5 candidate models by default. We set n=1

```{r}
dtree_resample %>%
  show_best(n=1)
```

**RMSE 0.87 STD_ERR = 0.015**

**From the plot and the tibble above, we see that the the model with treedepth =1 has the lowest rmse value with a mean of 1.19 and standard error of 0.018.**

We can also use the [`select_best()`](https://tune.tidymodels.org/reference/show_best.html) function to pull out the single set of hyperparameter values for our best decision tree model:

```{r}
#Selects best performing model
best_tree <- dtree_resample %>%
  select_best()

best_tree
```

##### Create final fit based on best model permutation and plotting predicted values from that final fit model

We can update (or "finalize") our workflow object `tree_wf` with the values from `select_best()`.

```{r}
dtree_final_wf <- 
  dtree_wf %>% 
  finalize_workflow(best_tree)

dtree_final_wf
```

```{r}
#Create workflow for fitting model to train predictions
dtree_final_fit <- 
  dtree_final_wf %>%
  fit(train) 
```

##### Calculating Residuals and Plotting Actual Vs. Predicted Values

```{r}
dtree_residuals <- dtree_final_fit %>%
  augment(train) %>% #use augment() to make predictions from train data
  select(c(.pred, egg_hen)) %>%
  mutate(.resid = egg_hen - .pred) #calculate residuals and make new row.

dtree_residuals
```

##### Predictions vs. Actual

```{r}
dtree_pred_plot <- ggplot(dtree_residuals, 
                          aes(x = egg_hen, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: Decision Tree", 
       x = "Egg_hen Outcome", 
       y = "Egg_hen Prediction")
dtree_pred_plot
```

##### Predictions vs. Residuals

```{r}
dtree_residual_plot <- ggplot(dtree_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: Decision Tree", 
       x = "Egg_hen Prediction", 
       y = "Residuals")
plot(dtree_residual_plot) #view plot
```

## 4. Regression Decision Tree

#### Regression Tree

```{r}
tree <- rpart(egg_hen ~prod_type, data = train)
rpart.plot(tree)

```

#### Predict

```{r}
p <- predict(tree, train)

```

#### RMSE and R2

```{r}
sqrt(mean(train$egg_hen - p))
(cor(train$egg_hen,p))
```

## 4. Naive Bayes????

<!--# I think take this out -->

```{r}
model <- naive_bayes(prod_type~ ., data = train, usekernel = T) 
 plot(model) 
```

## 5. Random Forest Model

#### Create function to detect cores for RFM computation

```{r}
cores <- parallel::detectCores()
cores
```

#### Specify Model

```{r}
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")
```

#### Creating Workflow

```{r}
rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(egg_rec_train)
```

#### Create Tuning Grid

```{r}
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6),
                        min_n = c(40,50,60), 
                        trees = c(500,1000)  )
```

#### Cross-validation

```{r}
rf_resample <- 
  rf_wf %>% 
  tune_grid(fold_egg_train,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(yardstick::rmse))
```

```{r}
rf_resample %>%
  collect_metrics()
```

#### Plot Model Performance

```{r}
#Plot of actual train data
rf_resample %>%
  autoplot()
```

#### Metrics: RMSE

```{r}
#Showing best performing tree models
rf_resample %>%
  show_best(n=1)

#Selects best performing model
best_rf <- rf_resample %>%
  select_best(method = "rmse")
```

**RMSE = 0.87 STD_ERR = 0.015**

#### Create Final Fit

```{r}
rf_final_wf <- 
  rf_wf %>% 
  finalize_workflow(best_rf)

#Create workflow for fitting model to train_data2 predictions
rf_final_fit <- 
  rf_final_wf %>%
  fit(train) 
```

#### Calculate Residuals

```{r}
rf_residuals <- rf_final_fit %>%
  augment(train) %>% #use augment() to make predictions from train data
  select(c(.pred, egg_hen)) %>%
  mutate(.resid = egg_hen - .pred) #calculate residuals and make new row.

rf_residuals
```

#### Model Predictions from Tuned Model vs Actual Outcomes

```{r}
rf_pred_plot <- ggplot(rf_residuals, 
                          aes(x = egg_hen, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: Random Forest", 
       x = "egg_hen Actual", 
       y = "egg_hen Prediction")
rf_pred_plot

rf_residual_plot <- ggplot(rf_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: Random Forest", 
       x = "Body Temperature Prediction", 
       y = "Residuals")
plot(rf_residual_plot) #view plot
```

## Final Assessment

In order to pick a model that I believe is best for my hypothesis, I must (1) understand the data and question (2) use visual plots, and (3) metrics (e.g. RMSE) to assess performance. Based on these three attributes, I believe that the one-way ANOVA is the best model to use in this case. I am working with a continuous outcome (eggs per hen) and a categorical independent variable (production type) and would like to assess the two means between hatching and table eggs. This model is sample and efficient, and properly addresses the questions. Additionally, the RMSE of the actual model (0.87) was lower than the null model (2.18) indicating a better performance.

While the Random Forest Model had a lower RMSE value (0.87), the plot produced depicts a fluctuation in performance with node size. The decision tree is a simple model, but in this case not very informative as we only had two outcomes.

#### ANOVA on Test Data

```{r}
#Model
anova_test<- 
  aov(egg_hen ~ prod_type, data = test)
summary(anova_test)

#Assess Performance
RSS<- c(crossprod(anova_test$residuals))
MSE<- RSS / length(anova_test$residuals)
RMSE <- sqrt(MSE)
RMSE
```

**RMSE = 0.99**

<!--# Worse... -->

#### Plot

```{r}
plot(anova_test, 1)
```

# Discussion

Summarize everything you did and found in a discussion. Of course, your Rmd file should contain commentary/documentation on everything you do for each step.

\
