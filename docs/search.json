[
  {
    "objectID": "tidytuesday_exercise2.html",
    "href": "tidytuesday_exercise2.html",
    "title": "Tidy Tuesday Exercise2",
    "section": "",
    "text": "To be filled :)"
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nlibrary(tidytuesdayR)\nlibrary(lubridate)\nlibrary(skimr)"
  },
  {
    "objectID": "tidytuesday_exercise.html#the-data",
    "href": "tidytuesday_exercise.html#the-data",
    "title": "Tidy Tuesday Exercise",
    "section": "The Data",
    "text": "The Data\n\ntuesdata <- tidytuesdayR::tt_load(2023, week = 7)\n\n\n    Downloading file 1 of 1: `age_gaps.csv`\n\nage_gaps <- tuesdata$age_gaps\n\n\nTake a look at the data\n\nglimpse(age_gaps)\n\nRows: 1,155\nColumns: 13\n$ movie_name         <chr> \"Harold and Maude\", \"Venus\", \"The Quiet American\", …\n$ release_year       <dbl> 1971, 2006, 2002, 1998, 2010, 1992, 2009, 1999, 199…\n$ director           <chr> \"Hal Ashby\", \"Roger Michell\", \"Phillip Noyce\", \"Joe…\n$ age_difference     <dbl> 52, 50, 49, 45, 43, 42, 40, 39, 38, 38, 36, 36, 35,…\n$ couple_number      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ actor_1_name       <chr> \"Ruth Gordon\", \"Peter O'Toole\", \"Michael Caine\", \"D…\n$ actor_2_name       <chr> \"Bud Cort\", \"Jodie Whittaker\", \"Do Thi Hai Yen\", \"T…\n$ character_1_gender <chr> \"woman\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", …\n$ character_2_gender <chr> \"man\", \"woman\", \"woman\", \"woman\", \"man\", \"woman\", \"…\n$ actor_1_birthdate  <date> 1896-10-30, 1932-08-02, 1933-03-14, 1930-09-17, 19…\n$ actor_2_birthdate  <date> 1948-03-29, 1982-06-03, 1982-10-01, 1975-11-08, 19…\n$ actor_1_age        <dbl> 75, 74, 69, 68, 81, 59, 62, 69, 57, 77, 59, 56, 65,…\n$ actor_2_age        <dbl> 23, 24, 20, 23, 38, 17, 22, 30, 19, 39, 23, 20, 30,…\n\nsummary(age_gaps)\n\n  movie_name         release_year    director         age_difference \n Length:1155        Min.   :1935   Length:1155        Min.   : 0.00  \n Class :character   1st Qu.:1997   Class :character   1st Qu.: 4.00  \n Mode  :character   Median :2004   Mode  :character   Median : 8.00  \n                    Mean   :2001                      Mean   :10.42  \n                    3rd Qu.:2012                      3rd Qu.:15.00  \n                    Max.   :2022                      Max.   :52.00  \n couple_number   actor_1_name       actor_2_name       character_1_gender\n Min.   :1.000   Length:1155        Length:1155        Length:1155       \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.398                                                           \n 3rd Qu.:2.000                                                           \n Max.   :7.000                                                           \n character_2_gender actor_1_birthdate    actor_2_birthdate     actor_1_age   \n Length:1155        Min.   :1889-04-16   Min.   :1906-10-06   Min.   :18.00  \n Class :character   1st Qu.:1953-05-16   1st Qu.:1965-03-25   1st Qu.:33.00  \n Mode  :character   Median :1964-10-03   Median :1974-07-30   Median :39.00  \n                    Mean   :1960-09-07   Mean   :1971-01-29   Mean   :40.64  \n                    3rd Qu.:1973-08-07   3rd Qu.:1982-04-07   3rd Qu.:47.00  \n                    Max.   :1996-06-01   Max.   :1996-11-11   Max.   :81.00  \n  actor_2_age   \n Min.   :17.00  \n 1st Qu.:25.00  \n Median :29.00  \n Mean   :30.21  \n 3rd Qu.:34.00  \n Max.   :68.00  \n\n\nIt looks like we have data from 1935-2022 that contains the movie, release date, director, each actors age and gender, and their birth-dates. For each couple, the age gap is defined in the age_difference column. Gender of character_1 is the older gender, while gender for character_2 is the younger gender in the relationship.\nThis dataset seems to be fairly clean, with consistent entries for each variable. So lets think about some analyses we can explore."
  },
  {
    "objectID": "tidytuesday_exercise.html#analysis-ideas",
    "href": "tidytuesday_exercise.html#analysis-ideas",
    "title": "Tidy Tuesday Exercise",
    "section": "Analysis Ideas",
    "text": "Analysis Ideas\n\n1. How has the age gap changed over the years?\n\n\n2. What is the most common age difference?\n\n\n3. Are age gaps where the male is older than the female more common? Or vice verse?\n\n\n4. Do we see a greater age gap between same-gender or opposite-gender couples?\n\n\n5. When do we start seeing the prevalence of same-gender relationships?"
  },
  {
    "objectID": "tidytuesday_exercise.html#how-has-age-gap-changed-over-the-years",
    "href": "tidytuesday_exercise.html#how-has-age-gap-changed-over-the-years",
    "title": "Tidy Tuesday Exercise",
    "section": "1. How has age gap changed over the years?",
    "text": "1. How has age gap changed over the years?\n\nSlim down Data\n\nd1 <- age_gaps %>%\n  select(release_year, age_difference)\n\nLets just look at the release year and age gap.\n\n\nAverage gap by year\n\nyear_avg<- d1 %>%\ngroup_by(release_year) %>% \nsummarize_if(is.numeric, mean) %>%\nungroup()\n\nBecause multiple movies came out in the same year, we are taking an average of the age gap per year.\n\n\nPlotting average age gap over the years\n\nyear_avg %>%\n  ggplot() + \n  geom_line(\n    aes(\n      x = release_year,\n      y = age_difference),\n    color = \"darkgreen\")+\n  geom_point(\n    aes(\n      x = release_year,\n      y = age_difference),\n    color = \"darkgreen\")+\n  theme_bw()+\n  labs(\n    x = \"Release Year\",\n    y = \"Age Gap (years)\",\n    title = \"Age gaps in movies from 1935-2022\") +\n  theme(\n    plot.title = element_text(hjust = 0.5))\n\n\n\n\nNothing really stands out here. We see a slight decrease in average age gap between ~1980-2018. the years 2020 and 2022 both had movies with age gaps >20 years (You Should Have Left, Mank, The Northman, and The Bubble)."
  },
  {
    "objectID": "tidytuesday_exercise.html#what-is-the-most-common-age-difference-1",
    "href": "tidytuesday_exercise.html#what-is-the-most-common-age-difference-1",
    "title": "Tidy Tuesday Exercise",
    "section": "2. What is the most common age difference?",
    "text": "2. What is the most common age difference?\n\nLet’s make a dataframe that contains the number of times an age gap is reported:\n\ndist<- age_gaps %>%\n  count(age_difference)\nsummary(dist)\n\n age_difference        n        \n Min.   : 0.00   Min.   : 1.00  \n 1st Qu.:11.25   1st Qu.: 2.25  \n Median :22.50   Median :16.50  \n Mean   :23.13   Mean   :25.11  \n 3rd Qu.:33.75   3rd Qu.:36.75  \n Max.   :52.00   Max.   :85.00  \n\n\n\n\nNow let’s plot it:\n\ndist %>%\n  ggplot() +\n  geom_point(\n    aes(\n      x = age_difference,\n      y = n),\n    color = \"darkblue\")+\n  geom_line(\n    aes(\n      x = age_difference,\n      y = n),\n    color = \"darkblue\")+\n  theme_bw()+\n scale_x_continuous(n.breaks=10)+\n  labs(\n    x = \"Age Gap (years)\",\n    y = \"Number of Times Age Gap Appears\",\n    title = \"Frequency of Age Gaps in Media from 1935-2022\" )+\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\nIt looks like an age gap of 2-3 years is most common. We also have an age gap of 52 years!"
  },
  {
    "objectID": "tidytuesday_exercise.html#are-age-gaps-where-the-male-is-older-than-the-female-more-common-or-vice-verse-1",
    "href": "tidytuesday_exercise.html#are-age-gaps-where-the-male-is-older-than-the-female-more-common-or-vice-verse-1",
    "title": "Tidy Tuesday Exercise",
    "section": "3. Are age gaps where the male is older than the female more common? Or vice verse?",
    "text": "3. Are age gaps where the male is older than the female more common? Or vice verse?\n\nMake new column that identifies the older male or female (character_1_gender refers to gender of older actor)\n\nage_gaps2<- age_gaps %>%\n  mutate(older = case_when(age_gaps$character_1_gender == \"woman\"~ \"Female\", # Older Female\n                           age_gaps$character_1_gender == \"man\" ~ \"Male\"))  # Older Male \n\n\n\nPlot Age difference over time by gender\n\nage_gaps2 %>% ggplot() + geom_point(\n  aes(\n    x = release_year,\n    y = age_difference,\n    color = older))+\n  theme_bw()+\n  labs(\n    x = \"Movie Release Year\",\n    y = \"Age Difference\",\n    title = \"Age Difference over Release Years by Gender\",\n    color = \"Older Actor\")+\n    theme(\n      plot.title = element_text(hjust = 0.5))\n\n\n\n\nA few things to note here. (1) It looks like older females in couples become more prevalent around the 1980s. (2) Despite a higher incidence of older females in couples, the age gap is relatively lower than that of older male couples. (3) Some of the movies include same-gender couples, which can make this graph misleading. In the case where the couple is woman-woman, older female will show up regardless. So:\n\n\nLet’s make new columns and dataframes for same and opposite gender couples\n\nage_gaps3<- age_gaps2 %>%\n  mutate(gender = case_when(\n    #Same-gender male couples\n    (age_gaps$character_1_gender == \"man\" & age_gaps$character_2_gender == \"man\") ~\"same\", \n    #Same-gender female couples\n    (age_gaps$character_1_gender == \"woman\" & age_gaps$character_2_gender == \"woman\") ~\"same\", \n    #Opposite-gender couples\n    (age_gaps$character_1_gender == \"woman\" & age_gaps$character_2_gender == \"man\") ~\"opposite\", \n     #Opposite-gender couples\n    (age_gaps$character_1_gender == \"man\" & age_gaps$character_2_gender == \"woman\") ~\"opposite\")) \n\n#New dataframes for same and opposite gender relationships \nage_same<- age_gaps3 %>%\n  filter(gender %in% \"same\")\n\nage_opp<- age_gaps3 %>%\n  filter(gender %in% \"opposite\")\n\n\nglimpse(age_same)\n\nRows: 23\nColumns: 15\n$ movie_name         <chr> \"Beginners\", \"A Single Man\", \"Freeheld\", \"Behind th…\n$ release_year       <dbl> 2010, 2009, 2015, 2013, 2009, 2009, 2008, 2012, 201…\n$ director           <chr> \"Mike Mills\", \"Tom Ford\", \"Peter Sollett\", \"Steven …\n$ age_difference     <dbl> 43, 29, 27, 26, 25, 18, 18, 17, 16, 14, 12, 10, 9, …\n$ couple_number      <dbl> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ actor_1_name       <chr> \"Christopher Plummer\", \"Colin Firth\", \"Julianne Moo…\n$ actor_2_name       <chr> \"Goran Visnjic\", \"Nicholas Hoult\", \"Elliot Page\", \"…\n$ character_1_gender <chr> \"man\", \"man\", \"woman\", \"man\", \"woman\", \"man\", \"man\"…\n$ character_2_gender <chr> \"man\", \"man\", \"woman\", \"man\", \"woman\", \"man\", \"man\"…\n$ actor_1_birthdate  <date> 1929-12-13, 1960-09-10, 1960-12-03, 1944-09-25, 19…\n$ actor_2_birthdate  <date> 1972-09-09, 1989-12-07, 1987-02-21, 1970-10-08, 19…\n$ actor_1_age        <dbl> 81, 49, 55, 69, 49, 49, 48, 54, 46, 44, 52, 47, 31,…\n$ actor_2_age        <dbl> 38, 20, 28, 43, 24, 31, 30, 37, 30, 30, 40, 37, 22,…\n$ older              <chr> \"Male\", \"Male\", \"Female\", \"Male\", \"Female\", \"Male\",…\n$ gender             <chr> \"same\", \"same\", \"same\", \"same\", \"same\", \"same\", \"sa…\n\n\nFrom this, we can see that 23 entries are same-gender couples. There is a lot going on with the above graph, so lets use a box plot to look at this data.\n\n\nLet’s make a box plot with only opposite gender couples\n\nage_opp %>% ggplot() + geom_boxplot(\n  aes(\n    x = older,\n    y = age_difference,\n    color = older))+\n  theme_bw()+\n  labs(\n    x = \"\",\n    y = \"Age Difference (years)\",\n    title = \"Age Difference by Gender of Opposite-gender Couples\",\n    color = \"Older Actor Gender\")+\n    theme(\n      plot.title = element_text(hjust = 0.5))\n\n\n\n\nThis is a better way to see the distribution of age gaps where either a male or female is the oldest in the relationship. We can see that on average, males are typically older than the females and the age gap is higher for older male-younger female relationships."
  },
  {
    "objectID": "tidytuesday_exercise.html#do-we-see-a-greater-age-gap-between-same-gender-or-opposite-gender-couples-1",
    "href": "tidytuesday_exercise.html#do-we-see-a-greater-age-gap-between-same-gender-or-opposite-gender-couples-1",
    "title": "Tidy Tuesday Exercise",
    "section": "4. Do we see a greater age gap between same-gender or opposite-gender couples?",
    "text": "4. Do we see a greater age gap between same-gender or opposite-gender couples?\n\nage_gaps3 %>% ggplot() + geom_boxplot(\n  aes(\n    x = gender,\n    y = age_difference,\n  color = gender))+\n  theme_bw()+\n  labs(\n    x = \"\",\n    y = \"Age Difference (years)\",\n    title = \"Age Gaps between Same/Opposite Gender Couples\")+\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"none\")\n\n\n\n\nOn average, there is a greater age gap in same gender couples, but it is important to note that there are only 23 entries for same-gender couples and 1132 entries for opposite gender couples. So this isn’t very informative."
  },
  {
    "objectID": "tidytuesday_exercise.html#how-has-the-prevalance-of-same-gender-relationships-in-movies-changed-over-the-years",
    "href": "tidytuesday_exercise.html#how-has-the-prevalance-of-same-gender-relationships-in-movies-changed-over-the-years",
    "title": "Tidy Tuesday Exercise",
    "section": "5: How has the prevalance of same-gender relationships in movies changed over the years?",
    "text": "5: How has the prevalance of same-gender relationships in movies changed over the years?\n\nSame-gender prevalence in film\n\nage_gaps3 %>% ggplot() + geom_point(\n  aes(\n    x = release_year,\n    y = age_difference,\n    color = gender,\n    alpha = gender))+\n  theme_bw()+\n  scale_color_manual(values = c(\"dif\" = \"grey\", \"same\" = \"darkred\")) +\n  labs(\n    x = \"Release Year\",\n    y = \"Age Difference (years)\",\n    title = \"Prevalance of Same-gender Relationships in Film\")+\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position  = \"none\")\n\n\n\n\nFrom our age_same data set, we can see that same-gender couples were documented starting in the year 1997 and that there are 23 recorded cases. We see a wide spread in age gap and that the prevalence of same-gender couples in the documented films increases starting after 1977."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My name is Nathan Greenslit and I am a second year Master’s student in the Department of Marine Sciences. In December of 2020, I graduated from Clemson University with a Bachelor’s in Biological Sciences. I currently work under the advisement of Dr. Erin Lipp, where my research addresses the cross section between environmental and public health. I am specifically interested in how environmental fluctuations can impact marine pathogen dynamics in coastal waterways, and how microbial blooms can impact both marine and human health.\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n\nI have a basal understanding of modern statistical approaches as I have completed a general statistics course during my undergraduate degree and recently completed a Quantitative Methods course past Fall. My first exposure to coding using R was last Spring, when I completed an intensive computation course that helped me gain familiarity with R Markdown, it’s syntax, and the creation of figures using ggplot2. I can comfortably filter through data and create simple figures.\nFrom this course, I hope to become more familiar with coding syntax, and expand upon my ability to produce figures, run models, create websites, work with data, and especially understand when and why we use specific statistical measures."
  },
  {
    "objectID": "aboutme.html#interesting-fact",
    "href": "aboutme.html#interesting-fact",
    "title": "About me",
    "section": "Interesting Fact:",
    "text": "Interesting Fact:\nBefore coming to UGA, I worked with a non-profit based in the Florida Keys that focused on scientific communication and coral restoration. My activities included leading a team of divers on a series of “outplant” dives, where we would directly restore species of coral (Staghorn, Elkhorn, Massive Starlet Coral) back out onto the reefs of the Upper Middle Keys.\n\n\n\n \n\n\n This is an image from one of my monitoring dives on Alligator Reef, Islamorada, FL. Our Staghorn outplants are accompanied by a loggerhead sea turtle and a barracuda."
  },
  {
    "objectID": "aboutme.html#explaining-machine-learning-in-5-levels-of-difficulty",
    "href": "aboutme.html#explaining-machine-learning-in-5-levels-of-difficulty",
    "title": "About me",
    "section": "Explaining Machine Learning in 5 Levels of Difficulty",
    "text": "Explaining Machine Learning in 5 Levels of Difficulty\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nThis is a really interesting video where a Computer Scientist explains machine learning at varying levels of difficulty, ranging from a child to an expert. I think that it is equally as important to disseminate your research as it is to conduct it!."
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "The plot that I am reproducing can be found on the fiverthirtyeight article “This year’s Boston Marathon was Slooooowww” from 2018. By graphing winning times from previous marathon’s by country, the plot shows that the winners (both men and women) had slower times in 2018 as compared to previous years. The dataset used can be found on the Boston Athletic Association website. The datasets (2) contain country, time, name, and year for both men and women respectively. Here is the original plot that I will be replicating:"
  },
  {
    "objectID": "visualization_exercise.html#libraries",
    "href": "visualization_exercise.html#libraries",
    "title": "Visualization Exercise",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(scales)\nlibrary(ggthemes) #To get plot themes\nlibrary(ggpubr)\nlibrary(patchwork) #To stack plots"
  },
  {
    "objectID": "visualization_exercise.html#load-datasets-for-men-and-women-times",
    "href": "visualization_exercise.html#load-datasets-for-men-and-women-times",
    "title": "Visualization Exercise",
    "section": "Load datasets for men and women times",
    "text": "Load datasets for men and women times\n\nmen<- read_csv(here(\"data\",\"men_times.csv\")) #Men's race times\nwomen<- read_csv(here(\"data\", \"women_times.csv\")) #Women's race times"
  },
  {
    "objectID": "visualization_exercise.html#data-altering",
    "href": "visualization_exercise.html#data-altering",
    "title": "Visualization Exercise",
    "section": "Data Altering",
    "text": "Data Altering\nLet’s preview the data:\n\nsummary(men)\n\n      year          name             country              time         \n Min.   :1897   Length:125         Length:125         Length:125       \n 1st Qu.:1928   Class :character   Class :character   Class1:hms       \n Median :1959   Mode  :character   Mode  :character   Class2:difftime  \n Mean   :1959                                         Mode  :numeric   \n 3rd Qu.:1990                                                          \n Max.   :2022                                                          \n\nsummary(women)\n\n      year          name             country              time         \n Min.   :1966   Length:56          Length:56          Length:56        \n 1st Qu.:1980   Class :character   Class :character   Class1:hms       \n Median :1994   Mode  :character   Mode  :character   Class2:difftime  \n Mean   :1994                                         Mode  :numeric   \n 3rd Qu.:2007                                                          \n Max.   :2022                                                          \n\n\nWhile we have winning times up to 2022, the graph was published in 2018, so we need to filter the dates up to that year. Here, I also created a new column that highlighted the primary countries for men and women. Using case_when(), if the country said: Japan (men), Ethiopia (women), United States, or Kenya, the new column repeated that name. Any other country was charactereized as NA in the new column.\n\nMake new column that singles out the three main countries of interest for men and women\n\nmen2<- men %>% \n  mutate(time2 = as.numeric(men$time)) %>% #Switched time from an interval (unknown) to numeric (seconds)\n  mutate(country2 = case_when(country == \"Kenya\"~\"Kenya\",\n                              country == \"United States\"~ \"United States\",\n                              country == \"Japan\"~\"Japan\"))%>%\n   filter(year %in% \"1897\":\"2018\")\n\nwomen2<- women %>% \n   mutate(time2 = as.numeric(women$time)) %>% #Switched time from an interval (unknown) to numeric (seconds)\n  mutate(country2 = case_when(country == \"Kenya\"~\"Kenya\",\n                              country == \"United States\"~ \"United States\",\n                              country == \"Ethiopia\"~\"Ethiopia\")) %>%\n  filter(year %in% \"1966\":\"2018\")\n\nmen_18<- men %>% #Single point of interest (2018)\n  filter(year %in% 2018)\n\nwomen_18<- women %>% #Single point of interest (2018)\n  filter(year %in% 2018)\n\nLet’s take a look at the result.\n\nprint(men2)\n\n# A tibble: 122 × 6\n    year name                           country       time     time2 country2   \n   <dbl> <chr>                          <chr>         <time>   <dbl> <chr>      \n 1  2018 \"Yuki Kawauchi\"                Japan         02:15:58  8158 Japan      \n 2  2017 \"Geoffrey Kirui\"               Kenya         02:09:37  7777 Kenya      \n 3  2016 \"Lemi Berhanu\"                 Ethiopia      02:12:45  7965 <NA>       \n 4  2015 \"Lelisa Desisa\"                Ethiopia      02:09:17  7757 <NA>       \n 5  2014 \"Mebrahtom \\\"Meb\\\" Keflezighi\" United States 02:08:37  7717 United Sta…\n 6  2013 \"Lelisa Desisa\"                Ethiopia      02:10:22  7822 <NA>       \n 7  2012 \"Wesley Korir\"                 Kenya         02:12:40  7960 Kenya      \n 8  2011 \"Geoffrey Mutai\"               Kenya         02:03:02  7382 Kenya      \n 9  2010 \"Robert Kiprono Cheruiyot\"     Kenya         02:05:52  7552 Kenya      \n10  2009 \"Deriba Merga\"                 Ethiopia      02:08:42  7722 <NA>       \n# … with 112 more rows\n\nprint(women2)\n\n# A tibble: 53 × 6\n    year name            country       time     time2 country2     \n   <dbl> <chr>           <chr>         <time>   <dbl> <chr>        \n 1  2018 Desiree Linden  United States 02:39:54  9594 United States\n 2  2017 Edna Kiplagat   Kenya         02:21:52  8512 Kenya        \n 3  2016 Atsede Baysa    Ethiopia      02:29:19  8959 Ethiopia     \n 4  2015 Caroline Rotich Kenya         02:24:55  8695 Kenya        \n 5  2014 Buzunesh Deba   Ethiopia      02:19:59  8399 Ethiopia     \n 6  2013 Rita Jeptoo     Kenya         02:26:25  8785 Kenya        \n 7  2012 Sharon Cherop   Kenya         02:31:50  9110 Kenya        \n 8  2011 Caroline Kilel  Kenya         02:22:36  8556 Kenya        \n 9  2010 Teyba Erkesso   Ethiopia      02:26:11  8771 Ethiopia     \n10  2009 Salina Kosgei   Kenya         02:32:16  9136 Kenya        \n# … with 43 more rows\n\n\n\n\nCreate lists for x axis\nThe figure displays the x axis in years as both 4 digits and 2 digits with a “`”. Here I manually create lists that will be used as axis labels downstream.\n\nmen_years<- c(\"1900\", \"'10\", \"'20\", \"'30\", \"'40\", \"'50\", \"'60\", \"'70\", \"'80\", \"'90\", \"2000\", \"'10\", \"'18\")\nwomen_years <- c(\"1970\",\"'80\", \"'90\", \"2000\", \"'10\", \"`18\" )"
  },
  {
    "objectID": "visualization_exercise.html#plotting",
    "href": "visualization_exercise.html#plotting",
    "title": "Visualization Exercise",
    "section": "Plotting",
    "text": "Plotting\n\nPlot Women\n\n#Basic plotting\nwom<- women2 %>% ggplot() +geom_point(\n  aes(x= year,\n      y = time,\n      color= country2),\n  alpha =0.5,\n  size = 3)+\ntheme_fivethirtyeight()+ #Theme is specific to their website\n  \n#Working with axis scales\n  scale_x_continuous(breaks = c(1970,1980, 1990, 2000,2010,2018), #Breaks will go from 1970-2018 by 10 years\n                     limits = c(1966,2018), #Limits set from 1966-2018\n                     labels = women_years) + #Here is that manual list for the x axis labels \n  scale_y_time(breaks = date_breaks(\"30 mins\"),\n               limits = c(\"7200\", \"12960\"))+ #times are in seconds \n  geom_vline(xintercept = 2018,alpha=0.3, #Adds dashed line to 2018\n             linetype = \"dotted\")+\n  \n#Add Labels/Arrows to Plot\n  annotate(\"text\", x = 2013, y =12555, label = \"WOMEN'S\", color = \"#36454F\", fontface=2, size = 6.4)+\n  annotate(\"text\", x = 2009, y =8300, label = \"Kenya\", color = \"#800080\", fontface =2) +\n  annotate(\"text\", x = 1979, y =11000, label = \"United\\nStates\", color = \"#6495ED\", fontface = 2)+\n  annotate(\"text\", x = 2000, y =9100, label = \"Ethiopia\", color = \"#228B22\", fontface = 2)+\n  annotate(\"text\", x = 2015, y =10600, label = \"Desiree\\nLinden\")+\n  annotate( geom = \"curve\", x = 2015, y = 10100, xend = 2017.3, yend = 9600, \n  curvature = .45, arrow = arrow(length = unit(2, \"mm\")))+\n  \n#Assign Colors by Country. Hex codes were Googled.\n  scale_color_manual(values = c(\"Ethiopia\" = \"#228B22\",\n                                \"United States\" = \"#6495ED\",\n                                \"Kenya\" = \"#800080\"))+\n  \n#Labels \n  labs(y = \"Winning time\",\n      title = \"A slower field at this year's Boston Marathon\",\n      subtitle = \"Finish time for winners of the Boston Marathon, by country\")+#,\n     # caption =\"'18\")+ #This was the only way I could think of adding 2018 the axis that had breaks of 10 years otherwise. \n  \n  \n#Work with Plot colors, fonts, and Label Positions\n  theme(legend.position = \"none\",\n        panel.background = element_rect(\"#E5E4E2\"),\n        plot.background = element_rect(\"#E5E4E2\"),\n        plot.title = element_text(color = \"black\",\n                                   hjust = -1.4),\n        plot.subtitle = element_text(hjust = -.41),\n        axis.title.y = element_text(color = \"#36454F\",\n                                     face= \"bold\")) +\n       # plot.caption = element_text(hjust = 0.96555, vjust = 8.1)) +\n  \n#Create new plot to overlay with point of interest (2018)\n  geom_point(data=women_18,\n           aes(x = year, y= time), \n           pch = 21, color =\"black\", #Outline point \n           size = 3,\n            alpha = 0.8)\n\nwom\n\n\n\n\n\n\nPlot Men\n\n#Basic Plotting\nmn<- men2 %>% ggplot() +geom_point(\n  aes(x= year,\n      y = time,\n      color= country2),\n  alpha = 0.5,\n  size = 3)+\ntheme_fivethirtyeight()+\n  \n#Working with axis scales\n  scale_x_continuous(breaks = c(1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2018), #Breaks go from 1900-2018 by 10 years\n                     limits = c(1897,2020), #Limits set from 1897-2020\n                    labels = men_years) + #Using list from above for x axis labels\n  scale_y_time(breaks = date_breaks(\"30 mins\"),\n               limits = c(\"7200\", \"10800\"))+ #Time in seconds\n  geom_vline(xintercept = 2018,\n             alpha=0.3,\n             linetype = \"dotted\") +\n  \n#Add Labels/Arrows to Plot\n  annotate(\"text\", x = 2011, y =10500, label = \"MEN'S\", color = \"#36454F\", fontface=2, size = 6.4)+\n  annotate(\"text\", x = 2000, y =7500, label = \"Kenya\", color = \"#800080\", fontface=2) +\n  annotate(\"text\", x = 1919, y =9500, label = \"United\\nStates\", color = \"#6495ED\", fontface=2)+\n  annotate(\"text\", x = 1960, y =7900, label = \"Japan\", color = \"red\", fontface =2)+\n  annotate(\"text\", x = 2010, y =9000, label = \"Yuki\\nKawauchi\")+\n  annotate( geom = \"curve\", x = 2010, y = 8750, xend = 2016.5, yend = 8150, \n    curvature = .45, arrow = arrow(length = unit(2, \"mm\")))+\n  \n#Working with Labels, etc\n   labs(y = \"Winning time\")+\n       #caption  = \"'18\")+\n  theme(legend.position = \"none\",\n        panel.background = element_rect(\"#E5E4E2\"),\n        plot.background = element_rect(\"#E5E4E2\"),\n       axis.title.y = element_text(color = \"#36454F\",\n                                   face = \"bold\"))+\n      # plot.caption = element_text(hjust = 0.952, vjust = 8.1)) +\n\n#Assign Colors by Country\n  scale_color_manual(values = c(\"Japan\" = \"red\",\n                                \"United States\" = \"#6495ED\",\n                                \"Kenya\" = \"#800080\"))+\n  \n\n#Overlay Plot with point of interest (2018)\n  geom_point(data=men_18, \n             aes(x = year, y= time), \n             pch = 21, color =\"black\",\n             alpha = 0.8,\n             size = 3) \n  \nmn\n\n\n\n\n\n\nFinal Plot\nHere I used ggarrange() to stack the plots on top of one another.\n\nfigure<- ggarrange(wom, mn + font(\"x.text\", size = 10),\n                    ncol = 1, nrow = 2) #Final plot is 1 column and 2 rows \n\n                \nfigure\n\n\n\n\nPretty close! A few things that I struggled with were getting the y axis tick labels to be displayed as “2:00:00” as opposed to “02:00:00”. I also could not get the dashed lines for `18 to line up for both graphs without skewing everything else."
  },
  {
    "objectID": "visualization_exercise.html#save-as-png",
    "href": "visualization_exercise.html#save-as-png",
    "title": "Visualization Exercise",
    "section": "Save as PNG",
    "text": "Save as PNG\n\npng(file = here(\"results\",\"plots\", \"marathon.png\"))\nfigure\ndev.off()\n\nquartz_off_screen \n                2"
  },
  {
    "objectID": "coding_exercise.html",
    "href": "coding_exercise.html",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "",
    "text": "Note: To make for a cleaner look, I used {r, message/results/echo/warning = FALSE} to hide the output."
  },
  {
    "objectID": "coding_exercise.html#examine-structure",
    "href": "coding_exercise.html#examine-structure",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "Examine Structure",
    "text": "Examine Structure\n\nhelp(gapminder) #Pulls up help page for gapminder data\nstr(gapminder) #Overview of data structure\nsummary(gapminder) #Summary of data\nclass(gapminder) #Determine type of object gap minder is (data frame)\nas_tibble(gapminder) #Displays dataframe as a readable tibble"
  },
  {
    "objectID": "coding_exercise.html#condense-dataset-to-african-countries",
    "href": "coding_exercise.html#condense-dataset-to-african-countries",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "Condense dataset to African Countries",
    "text": "Condense dataset to African Countries\n\nafricadata <- gapminder %>%\n  filter(continent %in% \"Africa\")\n\nstr(africadata) #Overview of africadata structure\nsummary(africadata) #Summary of #africadata. This is good for seeing NAs in data"
  },
  {
    "objectID": "coding_exercise.html#create-two-new-objects-infant-mortality-im-and-population-pop",
    "href": "coding_exercise.html#create-two-new-objects-infant-mortality-im-and-population-pop",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "Create two new objects: Infant Mortality (im) and Population (pop)",
    "text": "Create two new objects: Infant Mortality (im) and Population (pop)\n\nim <- \n  africadata %>%\n  select(infant_mortality, life_expectancy)\n\npop <- \n  africadata %>%\n  select(population, life_expectancy)\n\nstr(im)\nsummary(im)\n\nstr(pop)\nsummary(pop)"
  },
  {
    "objectID": "coding_exercise.html#plotting",
    "href": "coding_exercise.html#plotting",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "Plotting",
    "text": "Plotting\n\nLife Expectancy as a function of Infant Mortality\n\n\n\n\n\n\n\nLife Expectancy as a function of Population"
  },
  {
    "objectID": "coding_exercise.html#code-for-seeing-which-years-have-nas-for-infant-mortality",
    "href": "coding_exercise.html#code-for-seeing-which-years-have-nas-for-infant-mortality",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "Code for seeing which years have NAs for Infant Mortality",
    "text": "Code for seeing which years have NAs for Infant Mortality\n\nim_na<- africadata %>%\n  filter(is.na(infant_mortality))\nim_na\n\n1960-1981 and 2016 have NAs"
  },
  {
    "objectID": "coding_exercise.html#create-new-object-that-looks-at-the-year-2000",
    "href": "coding_exercise.html#create-new-object-that-looks-at-the-year-2000",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "Create new object that looks at the year 2000",
    "text": "Create new object that looks at the year 2000\n\ntwo_k<- africadata %>%\n  filter(year %in% \"2000\")\nstr(two_k)\nsummary(two_k)"
  },
  {
    "objectID": "coding_exercise.html#make-two-new-objects-for-the-year-2000",
    "href": "coding_exercise.html#make-two-new-objects-for-the-year-2000",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "Make two new objects for the year 2000",
    "text": "Make two new objects for the year 2000\n\nim2<- two_k %>%\nselect(infant_mortality, life_expectancy)\n\npop2<- two_k %>%\n  select(population, life_expectancy)"
  },
  {
    "objectID": "coding_exercise.html#plotting-1",
    "href": "coding_exercise.html#plotting-1",
    "title": "R Coding Exercise: Processing, Plotting, and Fitting Models",
    "section": "Plotting",
    "text": "Plotting\n\nLife Expectancy as a funtion of Infant Mortality in 2000\n\n\n\n\n\n\n\nLife Expectancy as a funtion of Population in 2000"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nathan Greenslit’s Website and Data Analysis Portfolio",
    "section": "",
    "text": "Hello!\n\nWelcome to my Website\nHere you will find some background information on myself as well as updates on current projects.\n\nPlease use the Menu Bar above to look around.\nHave fun!!!!"
  },
  {
    "objectID": "dataanalysis_exercise.html",
    "href": "dataanalysis_exercise.html",
    "title": "Data Analysis Exercise",
    "section": "",
    "text": "This dataset from the CDC contains the 10 leading causes of death in the US from 1999-2017.The data is derived from resident death certificates filed in the 50 states and D.C. The raw data includes: (1) Year, (2) 113 Cause Name, (3) Cause Name, (4) State, (5) Deaths, and (6) Age-adjusted Death Rate. The Cause Name variable is a simplified version of the 113 Cause Name (as assigned by the International Classification of Diseases) and the Age-adjusted Death rate (per 100,000 population) is calculated based on the US population in 2000. Death rates after 2010 are based on 2010 census.\nLink to Dataset: https://data.cdc.gov/NCHS/NCHS-Leading-Causes-of-Death-United-States/bi63-dtpu/data\nLoad Libraries\nLoad Data from raw data folder\nExamine Data\nCreate new dataframe “data2”: selecting variables of interest and renaming them Upper case letters and spaces can be a hassle\nWe have selected variables: Year, Cause of Death, State, and Deaths.\nUse filter() to look at Deaths in Georgia\nSo now we have a final dataset that contains causes of deaths from the years 1999-2017 in Georgia\nSave Cleaned data as RDS file in clean_data folder located in dataanalysis_exercise –> data –> clean_data\nSave Summary Table as RDS file in Results folder"
  },
  {
    "objectID": "dataanalysis_exercise.html#this-section-added-by-sara-benist",
    "href": "dataanalysis_exercise.html#this-section-added-by-sara-benist",
    "title": "Data Analysis Exercise",
    "section": "This section added by Sara Benist",
    "text": "This section added by Sara Benist\n###Load RDS file\n\nreadRDS(file = \"dataanalysis_exercise/data/clean_data/cleandata_file.rds\")\n\n# A tibble: 209 × 4\n    year cause                   state   deaths\n   <dbl> <chr>                   <chr>    <dbl>\n 1  2017 Unintentional injuries  Georgia   4712\n 2  2017 All causes              Georgia  83098\n 3  2017 Alzheimer's disease     Georgia   4290\n 4  2017 Stroke                  Georgia   4399\n 5  2017 CLRD                    Georgia   4866\n 6  2017 Diabetes                Georgia   2348\n 7  2017 Heart disease           Georgia  18389\n 8  2017 Influenza and pneumonia Georgia   1400\n 9  2017 Suicide                 Georgia   1451\n10  2017 Cancer                  Georgia  17135\n# … with 199 more rows\n\nreadRDS(file = \"dataanalysis_exercise/results/summarytable.rds\")\n\n        year     cause     state           deaths\nMin.    1999       209       209              847\n1st Qu. 2003 character character             1604\nMedian  2008 character character             3411\nMean    2008       209       209 11130.4545454545\n3rd Qu. 2013 character character            14032\nMax.    2017 character character            83098"
  },
  {
    "objectID": "dataanalysis_exercise.html#looking-at-the-data",
    "href": "dataanalysis_exercise.html#looking-at-the-data",
    "title": "Data Analysis Exercise",
    "section": "Looking at the data",
    "text": "Looking at the data\nTo make sure the data loaded correctly, we will look at the summary measures of the dataframe.\n\nsummary(ga)\n\n      year         cause              state               deaths     \n Min.   :1999   Length:209         Length:209         Min.   :  847  \n 1st Qu.:2003   Class :character   Class :character   1st Qu.: 1604  \n Median :2008   Mode  :character   Mode  :character   Median : 3411  \n Mean   :2008                                         Mean   :11130  \n 3rd Qu.:2013                                         3rd Qu.:14032  \n Max.   :2017                                         Max.   :83098  \n\nstr(ga)\n\ntibble [209 × 4] (S3: tbl_df/tbl/data.frame)\n $ year  : num [1:209] 2017 2017 2017 2017 2017 ...\n $ cause : chr [1:209] \"Unintentional injuries\" \"All causes\" \"Alzheimer's disease\" \"Stroke\" ...\n $ state : chr [1:209] \"Georgia\" \"Georgia\" \"Georgia\" \"Georgia\" ...\n $ deaths: num [1:209] 4712 83098 4290 4399 4866 ...\n\n\n\nSome simple exploratory analysis with plots\nLet’s look at a a series of boxplots for each year of only the All Cause deaths in Georgia.\n\n#filter for all causes and pull death column\nACdeaths <- ga %>% \n  filter(cause == \"All causes\") %>% \n  pull(deaths)\n\n#filter for all causes and pull year column\nACyear <- ga %>% \n  filter(cause == \"All causes\") %>% \n  pull(year)\n\n#create boxplot\nboxplot(formula = ACdeaths ~ ACyear)\n\n\n\n\nWhile not very informative, we can see that the death rate for All Causes are increasing with each year. Let’s look at a better representation of the data.\nFirst, I want to look at the top category of injuries for GA over 1999 to 2017. To do this, I will plot the number of deaths for each cause for each year.\n\n#scatter plot of year vs deaths by cause\nggplot(ga, aes(x = year, y = deaths, color = cause)) +\n  geom_point()+\n  geom_line()+\n  scale_y_continuous(trans = \"log\")+\n  scale_color_brewer(palette = \"Paired\")\n\n\n\n\nUnlike with the box plot, the all causes line is not increasing as drastically as the boxplot suggested.Looking at the separated causes of death, we can see from the plot that the top causes are heart disease and cancer in Georgia for this time period. One cause of death that I find interesting is the sharp increase in Alzheimer’s disease around 2013.\nWe can try to fit a model to the Alzheimer’s line to try to predict future trends.\n\n\nLinear model\n\n#filter for alzheimer's disease\nAD <- ga %>% filter(cause == \"Alzheimer's disease\")\n\n#create fit and summary\nfit <- lm(deaths ~ year, data = AD)\nsummary(fit)\n\n\nCall:\nlm(formula = deaths ~ year, data = AD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-804.16 -337.48   10.23  203.93  879.15 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -278310.97   38525.50  -7.224 1.42e-06 ***\nyear            139.67      19.19   7.280 1.29e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 458.1 on 17 degrees of freedom\nMultiple R-squared:  0.7571,    Adjusted R-squared:  0.7429 \nF-statistic:    53 on 1 and 17 DF,  p-value: 1.286e-06\n\n\nThe linear model suggests year is a valid predictor of the number of deaths from Alzheimer’s disease with an expected increase of 139.67 deaths per year."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html",
    "href": "fluanalysis/code/machinelearning.html",
    "title": "machinelearning",
    "section": "",
    "text": "library(here)\n\nhere() starts at /Users/nathangreenslit/Desktop/UGA/Spring 2023/MADA/nathangreenslit-MADA-portfolio\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rsample) #Data splitting\nlibrary(tidymodels)#Modeling\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.2     ✔ recipes      1.0.5\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.4     ✔ yardstick    1.1.0\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(rpart) #Model Fitting\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(ranger) #Model Fitting\nlibrary(glmnet) #Model Fitting\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-6\n\nlibrary(rpart.plot)  # for visualizing a decision tree\nlibrary(vip)         # for variable importance plots\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\n\n\n\ndata<- readRDS(here(\"fluanalysis\", \"data\", \"SypAct_clean.rds\"))"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#tree-model",
    "href": "fluanalysis/code/machinelearning.html#tree-model",
    "title": "machinelearning",
    "section": "Tree Model:",
    "text": "Tree Model:\n\nModel Specification\n\ntune_spec_dtree <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()) %>%\n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n\ntune_spec_dtree\n\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\nThink of tune() here as a placeholder. After the tuning process, we will select a single numeric value for each of these hyperparameters. For now, we specify our parsnip model object and identify the hyperparameters we will tune().\n\n\nWorkflow Definition\n\ndtree_wf <- workflow() %>%\n  add_model(tune_spec_dtree) %>%\n  add_recipe(bt_rec_train)\n\n\n\nTuning Grid Specification\nWe can create a regular grid of values to try using some convenience functions for each hyperparameter:\n\n#create a regular grid of values for using convenience functions for each hyperparameter.\ntree_grid_dtree <-\n  grid_regular(\n    cost_complexity(), \n    tree_depth(), \n    levels = 5)\n\ntree_grid_dtree\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             <dbl>      <int>\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# … with 15 more rows\n\n\n\n\nTuning using Cross-validation and tune_grid() function\n\ndtree_resample <- \n  dtree_wf %>% \n  tune_grid(\n    resamples = fold_bt_train,\n    grid = tree_grid_dtree)\n\n! Fold1, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\nOnce we have our tuning results, we can both explore them through visualization and then select the best result. The function collect_metrics() gives us a tidy tibble with all the results\n\ndtree_resample %>%\n  collect_metrics()\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             <dbl>      <int> <chr>   <chr>         <dbl> <int>    <dbl> <chr>  \n 1    0.0000000001          1 rmse    standard     1.19      25  0.0181  Prepro…\n 2    0.0000000001          1 rsq     standard     0.0361    25  0.00422 Prepro…\n 3    0.0000000178          1 rmse    standard     1.19      25  0.0181  Prepro…\n 4    0.0000000178          1 rsq     standard     0.0361    25  0.00422 Prepro…\n 5    0.00000316            1 rmse    standard     1.19      25  0.0181  Prepro…\n 6    0.00000316            1 rsq     standard     0.0361    25  0.00422 Prepro…\n 7    0.000562              1 rmse    standard     1.19      25  0.0181  Prepro…\n 8    0.000562              1 rsq     standard     0.0361    25  0.00422 Prepro…\n 9    0.1                   1 rmse    standard     1.21      25  0.0177  Prepro…\n10    0.1                   1 rsq     standard   NaN          0 NA       Prepro…\n# … with 40 more rows\n\n\n\nPlot Model using autoplot()\n\ndtree_resample %>%\n  collect_metrics() %>%\n  mutate(tree_depth = factor(tree_depth)) %>%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(linewidth = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\nWarning: Removed 5 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nShow and Select best performing models\nThe show_best() function shows us the top 5 candidate models by default. We set n=1\n\ndtree_resample %>%\n  show_best(n=1)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 1 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            <dbl>      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>       \n1    0.0000000001          1 rmse    standard    1.19    25  0.0181 Preprocesso…\n\n\nFrom the plot and the tibble above, we see that the the model with treedepth =1 has the lowest rmse value with a mean of 1.19 and standard error of 0.018.\nWe can also use the select_best() function to pull out the single set of hyperparameter values for our best decision tree model:\n\n#Selects best performing model\nbest_tree <- dtree_resample %>%\n  select_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001          1 Preprocessor1_Model01\n\n\n\n\nCreate final fit based on best model permutation and plotting predicted values from that final fit model\nWe can update (or “finalize”) our workflow object tree_wf with the values from select_best().\n\ndtree_final_wf <- \n  dtree_wf %>% \n  finalize_workflow(best_tree)\n\ndtree_final_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_ordinalscore()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 1\n\nComputational engine: rpart \n\n\n\n#Create workflow for fitting model to train predictions\ndtree_final_fit <- \n  dtree_final_wf %>%\n  fit(train) \n\n\n\nCalculating Residuals and Plotting Actual Vs. Predicted Values\n\ndtree_residuals <- dtree_final_fit %>%\n  augment(train) %>% #use augment() to make predictions from train data\n  select(c(.pred, BodyTemp)) %>%\n  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.\n\ndtree_residuals\n\n# A tibble: 508 × 3\n   .pred BodyTemp .resid\n   <dbl>    <dbl>  <dbl>\n 1  99.2     97.8 -1.44 \n 2  99.2     98.1 -1.14 \n 3  98.7     98.1 -0.591\n 4  98.7     98.2 -0.491\n 5  98.7     97.8 -0.891\n 6  98.7     98.2 -0.491\n 7  98.7     98.1 -0.591\n 8  99.2     98   -1.24 \n 9  99.2     97.7 -1.54 \n10  99.2     98.2 -1.04 \n# … with 498 more rows\n\n\n\n\nPredictions vs. Actual\n\ndtree_pred_plot <- ggplot(dtree_residuals, \n                          aes(x = BodyTemp, \n                              y = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Actual: Decision Tree\", \n       x = \"Body Temperature Outcome\", \n       y = \"Body Temperature Prediction\")\ndtree_pred_plot\n\n\n\n\n\n\nPredictions vs. Residuals\n\ndtree_residual_plot <- ggplot(dtree_residuals, \n                              aes(y = .resid, \n                                  x = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Residuals: Decision Tree\", \n       x = \"Body Temperature Prediction\", \n       y = \"Residuals\")\nplot(dtree_residual_plot) #view plot"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#lasso-model",
    "href": "fluanalysis/code/machinelearning.html#lasso-model",
    "title": "machinelearning",
    "section": "Lasso Model:",
    "text": "Lasso Model:\n\nSpecify Model\n\nlasso_mod <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n\nSetting mixture to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.\n\n\nCreate Workflow\n\nlasso_wf <- workflow() %>%\n  add_model(lasso_mod) %>%\n  add_recipe(bt_rec_train)\n\n\n\nCreating a Tuning Grid\n\nlasso_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))\n\n\n\nCross Validation with tune_grid()\n\nlasso_resample <- \n  lasso_wf %>%\n  tune_grid(resamples = fold_bt_train,\n            grid = lasso_grid,\n            control = control_grid(verbose = FALSE, save_pred = TRUE),\n            metrics = metric_set(rmse))\n\nlasso_resample %>%\n  collect_metrics()\n\n# A tibble: 30 × 7\n   penalty .metric .estimator  mean     n std_err .config              \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1 0.001   rmse    standard    1.18    25  0.0167 Preprocessor1_Model01\n 2 0.00127 rmse    standard    1.18    25  0.0167 Preprocessor1_Model02\n 3 0.00161 rmse    standard    1.18    25  0.0167 Preprocessor1_Model03\n 4 0.00204 rmse    standard    1.17    25  0.0167 Preprocessor1_Model04\n 5 0.00259 rmse    standard    1.17    25  0.0167 Preprocessor1_Model05\n 6 0.00329 rmse    standard    1.17    25  0.0167 Preprocessor1_Model06\n 7 0.00418 rmse    standard    1.17    25  0.0167 Preprocessor1_Model07\n 8 0.00530 rmse    standard    1.17    25  0.0167 Preprocessor1_Model08\n 9 0.00672 rmse    standard    1.17    25  0.0167 Preprocessor1_Model09\n10 0.00853 rmse    standard    1.17    25  0.0167 Preprocessor1_Model10\n# … with 20 more rows\n\n\n\nPlot Model Performance\n\nlr_plot <- \n  lasso_resample %>% \n  collect_metrics() %>% \n  ggplot(aes(x = penalty, y = mean)) + \n  geom_point() + \n  geom_line() +\n  scale_x_log10(labels = scales::label_number())\n\nlr_plot\n\n\n\n\n\n\nShowing and selecting best performing Models\n\n#Showing best performing tree models\nlasso_resample %>%\n  show_best(n=1)\n\n# A tibble: 1 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0574 rmse    standard    1.15    25  0.0170 Preprocessor1_Model18\n\n#Selects best performing model\nbest_lasso <- lasso_resample %>%\n  select_best()\n\nHere our RMSE = 1.15 and standard deviation = 0.017. Based on this metric, the Lasso model seems to have performed better than the Tree model. Let’s come back to this.\n\n\nCreating Final Fit based on based model permutation and plotting predicted values from that final fit model\n\nlasso_final_wf <- \n  lasso_wf %>% \n  finalize_workflow(best_lasso)\n\nlasso_final_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_ordinalscore()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.0573615251044868\n  mixture = 1\n\nComputational engine: glmnet \n\n#Create workflow for fitting model to train_data2 predictions\nlasso_final_fit <- \n  lasso_final_wf %>%\n  fit(train) \n\n\n\nCalculate Residuals\n\nlasso_residuals <- lasso_final_fit %>%\n  augment(train) %>% #use augment() to make predictions from train data\n  select(c(.pred, BodyTemp)) %>%\n  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.\n\nlasso_residuals\n\n# A tibble: 508 × 3\n   .pred BodyTemp .resid\n   <dbl>    <dbl>  <dbl>\n 1  98.7     97.8 -0.910\n 2  98.9     98.1 -0.750\n 3  98.5     98.1 -0.379\n 4  98.9     98.2 -0.667\n 5  98.7     97.8 -0.920\n 6  98.7     98.2 -0.473\n 7  98.3     98.1 -0.219\n 8  99.2     98   -1.24 \n 9  98.9     97.7 -1.16 \n10  98.9     98.2 -0.747\n# … with 498 more rows\n\n\n\n\nModel Predictions from tuned model vs actual outcomes\n\nlasso_pred_plot <- ggplot(lasso_residuals, \n                          aes(x = BodyTemp, \n                              y = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Actual: LASSO\", \n       x = \"Body Temperature Outcome\", \n       y = \"Body Temperature Prediction\")\nlasso_pred_plot\n\n\n\nlasso_residual_plot <- ggplot(lasso_residuals, \n                              aes(y = .resid, \n                                  x = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Residuals: LASSO\", \n       x = \"Body Temperature Prediction\", \n       y = \"Residuals\")\nplot(lasso_residual_plot) #view plot"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#random-forest",
    "href": "fluanalysis/code/machinelearning.html#random-forest",
    "title": "machinelearning",
    "section": "Random Forest",
    "text": "Random Forest\n\nCreate fxn to detect cores for RFM computation\n\ncores <- parallel::detectCores()\ncores\n\n[1] 8\n\n\n\n\nSpecify Model\n\nrf_mod <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores) %>% \n  set_mode(\"regression\")\n\n\n\nCreating Workflow\n\nrf_wf <- workflow() %>%\n  add_model(rf_mod) %>%\n  add_recipe(bt_rec_train)\n\n\n\nCreate Tuning Grid\n\nrf_grid  <- expand.grid(mtry = c(3, 4, 5, 6),\n                        min_n = c(40,50,60), \n                        trees = c(500,1000)  )\n\n\n\nCross-validation\n\nrf_resample <- \n  rf_wf %>% \n  tune_grid(fold_bt_train,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(rmse))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\nrf_resample %>%\n  collect_metrics()\n\n# A tibble: 25 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1     8    38 rmse    standard    1.16    25  0.0167 Preprocessor1_Model01\n 2    29    14 rmse    standard    1.20    25  0.0167 Preprocessor1_Model02\n 3    24     2 rmse    standard    1.22    25  0.0163 Preprocessor1_Model03\n 4    15    31 rmse    standard    1.17    25  0.0166 Preprocessor1_Model04\n 5     7    32 rmse    standard    1.16    25  0.0169 Preprocessor1_Model05\n 6    20     9 rmse    standard    1.20    25  0.0165 Preprocessor1_Model06\n 7    13     7 rmse    standard    1.19    25  0.0167 Preprocessor1_Model07\n 8     2    15 rmse    standard    1.17    25  0.0170 Preprocessor1_Model08\n 9    23    28 rmse    standard    1.18    25  0.0168 Preprocessor1_Model09\n10    23     4 rmse    standard    1.22    25  0.0164 Preprocessor1_Model10\n# … with 15 more rows\n\n\n\nPlot Model Performance\n\n#Plot of actual train data\nrf_resample %>%\n  autoplot()\n\n\n\n\n\n\nShowing and Selecting Best Performing Models\n\n#Showing best performing tree models\nrf_resample %>%\n  show_best(n=1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8    38 rmse    standard    1.16    25  0.0167 Preprocessor1_Model01\n\n#Selects best performing model\nbest_rf <- rf_resample %>%\n  select_best(method = \"rmse\")\n\nOur best model had an RMSE = 1.16 and standard error = 0.017\n\n\nCreate Final Fit\n\nrf_final_wf <- \n  rf_wf %>% \n  finalize_workflow(best_rf)\n\n#Create workflow for fitting model to train_data2 predictions\nrf_final_fit <- \n  rf_final_wf %>%\n  fit(train) \n\n\n\nCalculate Residuals\n\nrf_residuals <- rf_final_fit %>%\n  augment(train) %>% #use augment() to make predictions from train data\n  select(c(.pred, BodyTemp)) %>%\n  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.\n\nrf_residuals\n\n# A tibble: 508 × 3\n   .pred BodyTemp .resid\n   <dbl>    <dbl>  <dbl>\n 1  98.7     97.8 -0.854\n 2  98.6     98.1 -0.513\n 3  98.7     98.1 -0.643\n 4  98.7     98.2 -0.535\n 5  98.8     97.8 -1.02 \n 6  98.5     98.2 -0.336\n 7  98.3     98.1 -0.218\n 8  99.1     98   -1.12 \n 9  98.8     97.7 -1.07 \n10  98.8     98.2 -0.644\n# … with 498 more rows\n\n\n\n\nModel Predictions from Tuned Model vs Actual Outcomes\n\nrf_pred_plot <- ggplot(rf_residuals, \n                          aes(x = BodyTemp, \n                              y = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Actual: Random Forest\", \n       x = \"Body Temperature Actual\", \n       y = \"Body Temperature Prediction\")\nrf_pred_plot\n\n\n\nrf_residual_plot <- ggplot(rf_residuals, \n                              aes(y = .resid, \n                                  x = .pred)) + \n  geom_point() + \n  labs(title = \"Predictions vs Residuals: Random Forest\", \n       x = \"Body Temperature Prediction\", \n       y = \"Residuals\")\nplot(rf_residual_plot) #view plot"
  },
  {
    "objectID": "fluanalysis/code/exploration.html",
    "href": "fluanalysis/code/exploration.html",
    "title": "exploration",
    "section": "",
    "text": "library(tidyverse) #For plotting and wrangling\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(here) #Setting working directory \n\nhere() starts at /Users/nathangreenslit/Desktop/UGA/Spring 2023/MADA/nathangreenslit-MADA-portfolio\n\nlibrary(gtsummary) #For summary tables \n\n#Uighur"
  },
  {
    "objectID": "fluanalysis/code/exploration.html#load-data",
    "href": "fluanalysis/code/exploration.html#load-data",
    "title": "exploration",
    "section": "Load Data",
    "text": "Load Data\n\nd<- readRDS(here(\"fluanalysis\", \"data\", \"SypAct_clean.rds\"))"
  },
  {
    "objectID": "fluanalysis/code/exploration.html#produce-and-print-a-summary-table-for-both-body-temperature-and-nausea",
    "href": "fluanalysis/code/exploration.html#produce-and-print-a-summary-table-for-both-body-temperature-and-nausea",
    "title": "exploration",
    "section": "Produce and Print a Summary Table for both Body Temperature and Nausea",
    "text": "Produce and Print a Summary Table for both Body Temperature and Nausea\n\nSummary Stats\n\nsummary(d$BodyTemp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  97.20   98.20   98.50   98.94   99.30  103.10 \n\nsummary(d$Nausea)\n\n No Yes \n475 255 \n\n\n\n\nSummary of full data set\n\ntab<- \n  tbl_summary(d)\ntab\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      N = 7301\n    \n  \n  \n    Swollen Lymph Nodes\n312 (43%)\n    Chest Congestion\n407 (56%)\n    Chills/Sweats\n600 (82%)\n    Nasal Congestion\n563 (77%)\n    Sneeze\n391 (54%)\n    Fatigue\n666 (91%)\n    Subjective Fever\n500 (68%)\n    Headache\n615 (84%)\n    Weakness\n\n        None\n49 (6.7%)\n        Mild\n223 (31%)\n        Moderate\n338 (46%)\n        Severe\n120 (16%)\n    Cough Severity\n\n        None\n47 (6.4%)\n        Mild\n154 (21%)\n        Moderate\n357 (49%)\n        Severe\n172 (24%)\n    Myalgia\n\n        None\n79 (11%)\n        Mild\n213 (29%)\n        Moderate\n325 (45%)\n        Severe\n113 (15%)\n    Runny Nose\n519 (71%)\n    Abdominal Pain\n91 (12%)\n    Chest Pain\n233 (32%)\n    Diarrhea\n99 (14%)\n    Eye Pain\n113 (15%)\n    Sleeplessness\n415 (57%)\n    Itchy Eyes\n179 (25%)\n    Nausea\n255 (35%)\n    Ear Pain\n162 (22%)\n    Sore Throat\n611 (84%)\n    Breathlessness\n294 (40%)\n    Tooth Pain\n165 (23%)\n    Vomiting\n78 (11%)\n    Wheezing\n220 (30%)\n    BodyTemp\n98.50 (98.20, 99.30)\n  \n  \n  \n    \n      1 n (%); Median (IQR)"
  },
  {
    "objectID": "fluanalysis/code/exploration.html#create-histogram-for-body-temperature",
    "href": "fluanalysis/code/exploration.html#create-histogram-for-body-temperature",
    "title": "exploration",
    "section": "Create Histogram for Body Temperature",
    "text": "Create Histogram for Body Temperature\n\nd %>%\n  ggplot()+geom_histogram(\n    aes(\n      x = BodyTemp))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA majority of the body temperatures fall between 98-99°F"
  },
  {
    "objectID": "fluanalysis/code/exploration.html#lets-look-at-predictor-variables-for-our-outcomes-of-interest-body-temperature-and-nausea",
    "href": "fluanalysis/code/exploration.html#lets-look-at-predictor-variables-for-our-outcomes-of-interest-body-temperature-and-nausea",
    "title": "exploration",
    "section": "Let’s look at Predictor Variables for our outcomes of interest (Body Temperature and Nausea)",
    "text": "Let’s look at Predictor Variables for our outcomes of interest (Body Temperature and Nausea)\n\nBody Temperature\n\nWeakness and Body Temp\n\nd %>%\n  ggplot()+geom_boxplot(\n    aes(\n      x= Weakness,\n      y = BodyTemp))\n\n\n\n\n\n\nChills and Sweats and Body Temp\n\nd %>%\n  ggplot()+geom_boxplot(\n    aes(\n      x= ChillsSweats,\n      y = BodyTemp))\n\n\n\n\n\n\nFatigue and Body Temp\n\nd %>%\n  ggplot()+geom_boxplot(\n    aes(\n      x= Fatigue,\n      y = BodyTemp))\n\n\n\n\n\n\nFever and Body Temp\n\nd %>%\n  ggplot()+geom_boxplot(\n    aes(\n      x= SubjectiveFever,\n      y = BodyTemp))\n\n\n\n\nOn average, it looks like those that experienced severe weakness symptoms, chills and sweats, fatigue, and fever had higher body temperatures. We would expect to see this positive relationship in the latter- so fever will be our main predictor of interest for Body Temperature."
  },
  {
    "objectID": "fluanalysis/code/exploration.html#now-lets-look-at-some-predictor-variables-for-the-categorical-variable-nausea",
    "href": "fluanalysis/code/exploration.html#now-lets-look-at-some-predictor-variables-for-the-categorical-variable-nausea",
    "title": "exploration",
    "section": "Now let’s look at some predictor variables for the categorical variable (Nausea)",
    "text": "Now let’s look at some predictor variables for the categorical variable (Nausea)\n\nDiarrhea and Nausea\n\nd %>%\n  ggplot() + geom_count(\n    aes(\n      x = Nausea,\n      y = Fatigue))\n\n\n\n\n\n\nChills and Nausea\n\nd %>%\n  ggplot() + geom_count(\n    aes(\n      x = Nausea,\n      y = ChillsSweats))\n\n\n\n\n\n\nFatigue and Nausea\n\nd %>%\n  ggplot() + geom_count(\n    aes(\n      x = Nausea,\n      y = Fatigue))\n\n\n\n\n\n\nFever and Nausea\n\nd %>%\n  ggplot() + geom_count(\n    aes(\n      x = Nausea,\n      y = SubjectiveFever))\n\n\n\n\nFever, Fatigue, Chills, and Diarrhea all appear to have equal or no major positive relationship (Where x:nausea = Yes-Yes < Yes-No). Vomiting will be our predictor of interest for the categorical outcome (Nausea), as there was a large proportion that did not experience either Nausea or Vomiting, and a medium-proportion of those that experienced one symptom with the other."
  },
  {
    "objectID": "fluanalysis/code/wrangling.html",
    "href": "fluanalysis/code/wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(here)\n\nhere() starts at /Users/nathangreenslit/Desktop/UGA/Spring 2023/MADA/nathangreenslit-MADA-portfolio\n\nlibrary(recipes)#Creating unordered factors/ordered factors\n\n\nAttaching package: 'recipes'\n\nThe following object is masked from 'package:stringr':\n\n    fixed\n\nThe following object is masked from 'package:stats':\n\n    step"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#load-data",
    "href": "fluanalysis/code/wrangling.html#load-data",
    "title": "Wrangling",
    "section": "Load Data",
    "text": "Load Data\n\nd<- readRDS(here(\"fluanalysis\", \"data\", \"SympAct_Any_Pos.Rda\"))"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#remove-columns-of-non-interest",
    "href": "fluanalysis/code/wrangling.html#remove-columns-of-non-interest",
    "title": "Wrangling",
    "section": "Remove Columns of non-interest",
    "text": "Remove Columns of non-interest\n\nd1<- d %>%\n  select(-contains(c(\"FluA\", \"FluB\", \"Score\", \"Total\", \"Dxname\", \"Activity\", \"Unique.Visit\"))) %>% #Removes Columns of non-interest \ndrop_na() #Drop NAs\n\nNow we have a data set with no NAs and presence/absence of flu symptoms (categorical), and body temperature (continuous)"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#remove-variables-with-multiple-levels-yes-no",
    "href": "fluanalysis/code/wrangling.html#remove-variables-with-multiple-levels-yes-no",
    "title": "Wrangling",
    "section": "Remove Variables with multiple levels/ Yes-No",
    "text": "Remove Variables with multiple levels/ Yes-No\n\nd2<- \n  d1 %>%\n  select(!c(WeaknessYN, CoughYN, MyalgiaYN, CoughYN2))"
  },
  {
    "objectID": "fluanalysis/code/wrangling.html#remove-binary-predictors-that-have50-entries-in-one-category",
    "href": "fluanalysis/code/wrangling.html#remove-binary-predictors-that-have50-entries-in-one-category",
    "title": "Wrangling",
    "section": "Remove Binary predictors that have<50 entries in one category",
    "text": "Remove Binary predictors that have<50 entries in one category\n\nCheck Data set\n\nsummary(d2)\n\n SwollenLymphNodes ChestCongestion ChillsSweats NasalCongestion Sneeze   \n No :418           No :323         No :130      No :167         No :339  \n Yes:312           Yes:407         Yes:600      Yes:563         Yes:391  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Fatigue   SubjectiveFever Headache      Weakness    CoughIntensity\n No : 64   No :230         No :115   None    : 49   None    : 47   \n Yes:666   Yes:500         Yes:615   Mild    :223   Mild    :154   \n                                     Moderate:338   Moderate:357   \n                                     Severe  :120   Severe  :172   \n                                                                   \n                                                                   \n     Myalgia    RunnyNose AbPain    ChestPain Diarrhea  EyePn     Insomnia \n None    : 79   No :211   No :639   No :497   No :631   No :617   No :315  \n Mild    :213   Yes:519   Yes: 91   Yes:233   Yes: 99   Yes:113   Yes:415  \n Moderate:325                                                              \n Severe  :113                                                              \n                                                                           \n                                                                           \n ItchyEye  Nausea    EarPn     Hearing   Pharyngitis Breathless ToothPn  \n No :551   No :475   No :568   No :700   No :119     No :436    No :565  \n Yes:179   Yes:255   Yes:162   Yes: 30   Yes:611     Yes:294    Yes:165  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Vision    Vomit     Wheeze       BodyTemp     \n No :711   No :652   No :510   Min.   : 97.20  \n Yes: 19   Yes: 78   Yes:220   1st Qu.: 98.20  \n                               Median : 98.50  \n                               Mean   : 98.94  \n                               3rd Qu.: 99.30  \n                               Max.   :103.10  \n\n\nWe can see that Vision and Hearing have <50 entries for one category. Let’s remove them.\n\n\nRemove Vision and Hearing\n\nd3<- \n  d2 %>%\n  select(!c(Vision, Hearing))\n\nNow we have a dataframe with 730 observations and 26 variables.\n\n\nSave RDS\n\nsaveRDS(d3, file= here(\"fluanalysis\", \"data\", \"SypAct_clean.rds\"))"
  },
  {
    "objectID": "fluanalysis/code/code_README.html",
    "href": "fluanalysis/code/code_README.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "wrangling.qmd take the data set and removes columns of non interest.\nexploration.qmd includes code that explored variables of specific interest that could predict our outcomes of interest: Body Temperature and Nausea. After looking through the variables, Fever was chosen as a predictor variable for the former, and Vomit was chosen for the latter.\nfitting.qmd includes code that runs 4 models: 1. A Linear model examining the relationship between Body Temperature and Fever 2. A linear model examining the relationship between Body Temperature and all variables 3. A logistic model examining the relationship between Nausea and Vomit 4. A logistic model examining the relationship between Nausea and all variables."
  },
  {
    "objectID": "fluanalysis/code/fitting.html",
    "href": "fluanalysis/code/fitting.html",
    "title": "Fitting Statistical Models",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(here)\n\nhere() starts at /Users/nathangreenslit/Desktop/UGA/Spring 2023/MADA/nathangreenslit-MADA-portfolio\n\nlibrary(tidymodels)  # for the parsnip package, along with the rest of tidymodels\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.2     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.4     ✔ yardstick    1.1.0\n✔ recipes      1.0.5     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(parsnip) #Had to load this separately due to error message\n\n# Helper packages for visualizatiob\nlibrary(readr)       # for importing data\nlibrary(broom.mixed) # for converting bayesian models to tidy tibbles\nlibrary(dotwhisker)  # for visualizing regression results\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\n#Libraries for model performance comparrison \nlibrary(performance)\n\n\nAttaching package: 'performance'\n\nThe following objects are masked from 'package:yardstick':\n\n    mae, rmse\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi"
  },
  {
    "objectID": "fluanalysis/code/fitting.html#load-data",
    "href": "fluanalysis/code/fitting.html#load-data",
    "title": "Fitting Statistical Models",
    "section": "1. Load Data",
    "text": "1. Load Data\n\nd<- readRDS(here(\"fluanalysis\", \"data\", \"SypAct_clean.rds\"))"
  },
  {
    "objectID": "fluanalysis/code/fitting.html#fitting-a-linear-model-to-the-continuous-outcome-body-temperature-using-only-the-main-predictor-of-interest-fever.",
    "href": "fluanalysis/code/fitting.html#fitting-a-linear-model-to-the-continuous-outcome-body-temperature-using-only-the-main-predictor-of-interest-fever.",
    "title": "Fitting Statistical Models",
    "section": "2. Fitting a linear model to the continuous outcome (Body temperature) using only the main predictor of interest (Fever).",
    "text": "2. Fitting a linear model to the continuous outcome (Body temperature) using only the main predictor of interest (Fever).\n\nDefine Linear Regression\n\nlinear_reg() %>%\n  set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nlm_mod<- linear_reg()\n\n\n\nTrain model to data\n\nlm_fit <- \n  lm_mod %>% \n  fit(BodyTemp ~ SubjectiveFever, data = d)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ SubjectiveFever, data = data)\n\nCoefficients:\n       (Intercept)  SubjectiveFeverYes  \n           98.5739              0.5273  \n\n\n\n\nExamine output\n\ntidy(lm_fit)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic      p.value\n  <chr>                 <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)          98.6      0.0773   1276.   0           \n2 SubjectiveFeverYes    0.527    0.0934      5.65 0.0000000233\n\n\n\n\nPlot\n\ntidy(lm_fit) %>% \n  dwplot(dot_args = list(size = 2, color = \"black\"),\n         whisker_args = list(color = \"black\"),\n         vline = geom_vline(xintercept = 0, colour = \"grey50\", linetype = 2))\n\n\n\nglance(lm_fit)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0420  0.0407  1.17    31.9 2.33e-8     1 -1151. 2307. 2321.   1000.     728\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual"
  },
  {
    "objectID": "fluanalysis/code/fitting.html#fitting-another-linear-model-to-the-continuous-outcome-body-temperature-using-all-predictors-of-interest.",
    "href": "fluanalysis/code/fitting.html#fitting-another-linear-model-to-the-continuous-outcome-body-temperature-using-all-predictors-of-interest.",
    "title": "Fitting Statistical Models",
    "section": "3. Fitting another linear model to the continuous outcome (Body Temperature) using all predictors of interest.",
    "text": "3. Fitting another linear model to the continuous outcome (Body Temperature) using all predictors of interest.\n\nDefine Linear Regression\n\nlm_mod_all<- linear_reg()\n\n\n\nTrain model to data\n\nlm_fit_all <- \n  lm_mod_all %>% \n  fit(BodyTemp ~ ., data = d)\nlm_fit_all\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ ., data = data)\n\nCoefficients:\n           (Intercept)    SwollenLymphNodesYes      ChestCongestionYes  \n             97.932520               -0.166066                0.102608  \n       ChillsSweatsYes      NasalCongestionYes               SneezeYes  \n              0.191826               -0.221568               -0.372709  \n            FatigueYes      SubjectiveFeverYes             HeadacheYes  \n              0.272509                0.436694                0.004977  \n          WeaknessMild        WeaknessModerate          WeaknessSevere  \n              0.005517                0.087801                0.351281  \n    CoughIntensityMild  CoughIntensityModerate    CoughIntensitySevere  \n              0.359527                0.261469                0.287089  \n           MyalgiaMild         MyalgiaModerate           MyalgiaSevere  \n              0.156342               -0.028450               -0.136091  \n          RunnyNoseYes               AbPainYes            ChestPainYes  \n             -0.064666                0.015375                0.100308  \n           DiarrheaYes                EyePnYes             InsomniaYes  \n             -0.152344                0.128786               -0.005824  \n           ItchyEyeYes               NauseaYes                EarPnYes  \n             -0.003889               -0.033246                0.111125  \n        PharyngitisYes           BreathlessYes              ToothPnYes  \n              0.315991                0.086379               -0.035497  \n              VomitYes               WheezeYes  \n              0.160053               -0.034654  \n\n\n\n\nExamine output\n\ntidy(lm_fit_all)\n\n# A tibble: 32 × 5\n   term                 estimate std.error statistic   p.value\n   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)          97.9        0.304   323.     0        \n 2 SwollenLymphNodesYes -0.166      0.0920   -1.81   0.0714   \n 3 ChestCongestionYes    0.103      0.0971    1.06   0.291    \n 4 ChillsSweatsYes       0.192      0.127     1.51   0.131    \n 5 NasalCongestionYes   -0.222      0.113    -1.95   0.0512   \n 6 SneezeYes            -0.373      0.0980   -3.80   0.000156 \n 7 FatigueYes            0.273      0.161     1.70   0.0901   \n 8 SubjectiveFeverYes    0.437      0.103     4.25   0.0000244\n 9 HeadacheYes           0.00498    0.125     0.0397 0.968    \n10 WeaknessMild          0.00552    0.189     0.0292 0.977    \n# … with 22 more rows\n\n\n\n\nPlot\n\ntidy(lm_fit_all) %>% \n  dwplot(dot_args = list(size = 2, color = \"black\"),\n         whisker_args = list(color = \"black\"),\n         vline = geom_vline(xintercept = 0, colour = \"grey50\", linetype = 2))"
  },
  {
    "objectID": "fluanalysis/code/fitting.html#compare-the-model-results-for-the-model-with-just-the-main-predictor-and-all-predictors.",
    "href": "fluanalysis/code/fitting.html#compare-the-model-results-for-the-model-with-just-the-main-predictor-and-all-predictors.",
    "title": "Fitting Statistical Models",
    "section": "4. Compare the model results for the model with just the main predictor and all predictors.",
    "text": "4. Compare the model results for the model with just the main predictor and all predictors.\n\nAssess Performance\n\ncheck_model(lm_fit$fit)\n\n\n\ncheck_model(lm_fit_all$fit)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\nCompare Performance\n\ncompare_performance(lm_fit,lm_fit_all)\n\n# Comparison of Model Performance Indices\n\nName       | Model |  AIC (weights) | AICc (weights) |  BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n---------------------------------------------------------------------------------------------------------\nlm_fit     |   _lm | 2307.1 (0.060) | 2307.1 (0.239) | 2320.9 (>.999) | 0.042 |     0.041 | 1.170 | 1.172\nlm_fit_all |   _lm | 2301.6 (0.940) | 2304.8 (0.761) | 2453.1 (<.001) | 0.124 |     0.085 | 1.119 | 1.144\n\n\nThere does not seem to be a major difference in model performance between the uni and multivariate models. Both have a similar observance/predicted line path and Normality of Residuals. The multivariate model does have a “higher” R2 and lower RMSE value."
  },
  {
    "objectID": "fluanalysis/code/fitting.html#fitting-a-logistic-model-to-the-categorical-outcome-nausea-using-only-the-main-predictor-of-interest-vomit.",
    "href": "fluanalysis/code/fitting.html#fitting-a-logistic-model-to-the-categorical-outcome-nausea-using-only-the-main-predictor-of-interest-vomit.",
    "title": "Fitting Statistical Models",
    "section": "5. Fitting a logistic model to the categorical outcome (Nausea) using only the main predictor of interest (Vomit).",
    "text": "5. Fitting a logistic model to the categorical outcome (Nausea) using only the main predictor of interest (Vomit).\n\n#Make model\nlog_mod <- logistic_reg() %>% \n  set_engine(\"glm\")\n\n#Train Model\nglm_fit <- \n  log_mod %>% \n  fit(Nausea ~ Vomit, data = d)\nglm_fit \n\nparsnip model object\n\n\nCall:  stats::glm(formula = Nausea ~ Vomit, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)     VomitYes  \n    -0.8811       2.4010  \n\nDegrees of Freedom: 729 Total (i.e. Null);  728 Residual\nNull Deviance:      944.7 \nResidual Deviance: 862  AIC: 866\n\n\n\nExamine model output\n\ntidy(glm_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -0.881    0.0861    -10.2  1.32e-24\n2 VomitYes       2.40     0.307       7.81 5.63e-15\n\n\n\n\nPlot\n\ntidy(glm_fit) %>% \n  dwplot(dot_args = list(size = 2, color = \"black\"),\n         whisker_args = list(color = \"black\"),\n         vline = geom_vline(xintercept = 0, colour = \"grey50\", linetype = 2))"
  },
  {
    "objectID": "fluanalysis/code/fitting.html#fits-another-logistic-model-to-the-categorical-outcome-using-all-important-predictors-of-interest.",
    "href": "fluanalysis/code/fitting.html#fits-another-logistic-model-to-the-categorical-outcome-using-all-important-predictors-of-interest.",
    "title": "Fitting Statistical Models",
    "section": "5. Fits another logistic model to the categorical outcome using all (important) predictors of interest.",
    "text": "5. Fits another logistic model to the categorical outcome using all (important) predictors of interest.\n\n#Make Model\nlog_mod_all <- logistic_reg() %>% \n  set_engine(\"glm\")\n\n#Train Model\nglm_fit_all <- \n  log_mod_all %>% \n  fit(Nausea ~ ., data = d)\nglm_fit_all \n\nparsnip model object\n\n\nCall:  stats::glm(formula = Nausea ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n           (Intercept)    SwollenLymphNodesYes      ChestCongestionYes  \n             0.2073331              -0.2468466               0.2652914  \n       ChillsSweatsYes      NasalCongestionYes               SneezeYes  \n             0.2905732               0.4400651               0.1650192  \n            FatigueYes      SubjectiveFeverYes             HeadacheYes  \n             0.2309581               0.2550192               0.3374915  \n          WeaknessMild        WeaknessModerate          WeaknessSevere  \n            -0.1203258               0.3243767               0.8296720  \n    CoughIntensityMild  CoughIntensityModerate    CoughIntensitySevere  \n            -0.3489315              -0.5080112              -1.0969293  \n           MyalgiaMild         MyalgiaModerate           MyalgiaSevere  \n            -0.0002988               0.2001406               0.1296120  \n          RunnyNoseYes               AbPainYes            ChestPainYes  \n             0.0526415               0.9415252               0.0715321  \n           DiarrheaYes                EyePnYes             InsomniaYes  \n             1.0647853              -0.3277246               0.0841010  \n           ItchyEyeYes                EarPnYes          PharyngitisYes  \n            -0.0526911              -0.1654360               0.2915358  \n         BreathlessYes              ToothPnYes                VomitYes  \n             0.5306311               0.4801514               2.4471603  \n             WheezeYes                BodyTemp  \n            -0.2758144              -0.0313649  \n\nDegrees of Freedom: 729 Total (i.e. Null);  698 Residual\nNull Deviance:      944.7 \nResidual Deviance: 752.1    AIC: 816.1\n\n\n\nExamine Output\n\ntidy(glm_fit_all)\n\n# A tibble: 32 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)             0.207     7.81     0.0265  0.979 \n 2 SwollenLymphNodesYes   -0.247     0.196   -1.26    0.207 \n 3 ChestCongestionYes      0.265     0.210    1.26    0.207 \n 4 ChillsSweatsYes         0.291     0.287    1.01    0.311 \n 5 NasalCongestionYes      0.440     0.253    1.74    0.0822\n 6 SneezeYes               0.165     0.210    0.787   0.431 \n 7 FatigueYes              0.231     0.371    0.622   0.534 \n 8 SubjectiveFeverYes      0.255     0.223    1.14    0.253 \n 9 HeadacheYes             0.337     0.285    1.18    0.236 \n10 WeaknessMild           -0.120     0.447   -0.269   0.788 \n# … with 22 more rows\n\n\n\n\nPlot\n\ntidy(glm_fit_all) %>% \n  dwplot(dot_args = list(size = 2, color = \"black\"),\n         whisker_args = list(color = \"black\"),\n         vline = geom_vline(xintercept = 0, colour = \"grey50\", linetype = 2))"
  },
  {
    "objectID": "fluanalysis/code/fitting.html#compares-the-model-results-for-the-categorical-model-with-just-the-main-predictor-and-all-predictors.",
    "href": "fluanalysis/code/fitting.html#compares-the-model-results-for-the-categorical-model-with-just-the-main-predictor-and-all-predictors.",
    "title": "Fitting Statistical Models",
    "section": "6. Compares the model results for the categorical model with just the main predictor and all predictors.",
    "text": "6. Compares the model results for the categorical model with just the main predictor and all predictors.\n\nAssess Performance\n\ncheck_model(glm_fit$fit)\n\n\n\ncheck_model(glm_fit_all$fit)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\nCompare Performance\n\ncompare_performance(glm_fit,glm_fit_all)\n\n# Comparison of Model Performance Indices\n\nName        | Model | AIC (weights) | AICc (weights) | BIC (weights) | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n-------------------------------------------------------------------------------------------------------------------------------------------------\nglm_fit     |  _glm | 866.0 (<.001) |  866.0 (<.001) | 875.2 (>.999) |     0.117 | 0.448 | 1.088 |    0.590 |  -123.912 |           0.019 | 0.599\nglm_fit_all |  _glm | 816.1 (>.999) |  819.2 (>.999) | 963.1 (<.001) |     0.246 | 0.415 | 1.038 |    0.515 |      -Inf |           0.002 | 0.657\n\n\nIn this case, it looks like the multivariate analysis had a better performance than the uni-variate. While both had a good observed/predicted line path, the former had better Normality of Residuals."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "modeleval",
    "section": "",
    "text": "library(here)\n\nhere() starts at /Users/nathangreenslit/Desktop/UGA/Spring 2023/MADA/nathangreenslit-MADA-portfolio\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rsample) #Data splitting\nlibrary(tidymodels)#Modeling\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.2     ✔ recipes      1.0.5\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.4     ✔ yardstick    1.1.0\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n\n\n\n\n\nd<- readRDS(here(\"fluanalysis\", \"data\", \"SypAct_clean.rds\"))"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#test-data",
    "href": "fluanalysis/code/modeleval.html#test-data",
    "title": "modeleval",
    "section": "Test Data",
    "text": "Test Data\n\nROC Curve\n\nflu_aug_test %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>% #> For binary classification, the first factor level is assumed to be the event. Use the argument `event_level = \"second\"` to alter this as needed.\n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>.\n\n\n\n\n\n\n\nArea Under the Curve\n\nflu_aug_test %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.731"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#train-data",
    "href": "fluanalysis/code/modeleval.html#train-data",
    "title": "modeleval",
    "section": "Train Data",
    "text": "Train Data\n\nROC Curve\n\nflu_aug_train %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>% \n  autoplot()\n\n\n\n\n\n\nArea Under the Curve\n\nflu_aug_train %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.785"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#test-data-1",
    "href": "fluanalysis/code/modeleval.html#test-data-1",
    "title": "modeleval",
    "section": "Test Data",
    "text": "Test Data\n\nROC Curve\n\nvom_aug_test %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>% \n  autoplot()\n\n\n\n\n\n\nArea Under the Curve\n\nvom_aug_test %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.578"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#train-data-1",
    "href": "fluanalysis/code/modeleval.html#train-data-1",
    "title": "modeleval",
    "section": "Train Data",
    "text": "Train Data\n\nROC Curve\n\nvom_aug_train %>% \n  roc_curve(truth = Nausea, .pred_Yes, event_level = \"second\") %>% \n  autoplot()\n\n\n\n\n\n\nArea Under the Curve\n\nvom_aug_train %>% \n  roc_auc(truth = Nausea, .pred_Yes, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.622"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#making-the-recipe-training",
    "href": "fluanalysis/code/modeleval.html#making-the-recipe-training",
    "title": "modeleval",
    "section": "Making the Recipe [Training]",
    "text": "Making the Recipe [Training]\n\nset.seed(321)\n#Creating the recipe \nBT_recipe<- recipe(BodyTemp ~., data=train_data)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#workflow-creation-and-prediction-training",
    "href": "fluanalysis/code/modeleval.html#workflow-creation-and-prediction-training",
    "title": "modeleval",
    "section": "Workflow Creation and Prediction [Training]",
    "text": "Workflow Creation and Prediction [Training]\n\n#Now let's set or define our model\nlr_BT<- linear_reg() %>%\n  set_engine(\"lm\")\n\n#Creating Workflow\nBT_WF<- workflow() %>% \n  add_model (lr_BT) %>%\n  add_recipe(BT_recipe)\n\n#Creation of Single Function\nBT_fit<- BT_WF %>% \n  fit(data= train_data)\n\n#Extracting \nBT_fit %>%\n  extract_fit_parsnip() %>%\n  tidy()\n\n# A tibble: 32 × 5\n   term                 estimate std.error statistic    p.value\n   <chr>                   <dbl>     <dbl>     <dbl>      <dbl>\n 1 (Intercept)           97.7        0.344   284.    0         \n 2 SwollenLymphNodesYes  -0.190      0.108    -1.76  0.0789    \n 3 ChestCongestionYes     0.146      0.115     1.27  0.203     \n 4 ChillsSweatsYes        0.184      0.148     1.24  0.214     \n 5 NasalCongestionYes    -0.182      0.136    -1.34  0.180     \n 6 SneezeYes             -0.474      0.113    -4.18  0.0000338 \n 7 FatigueYes             0.362      0.187     1.94  0.0529    \n 8 SubjectiveFeverYes     0.564      0.118     4.79  0.00000223\n 9 HeadacheYes            0.0675     0.150     0.448 0.654     \n10 WeaknessMild           0.0756     0.211     0.358 0.720     \n# … with 22 more rows\n\n#Predicting \npredict(BT_fit, train_data)\n\n# A tibble: 547 × 1\n   .pred\n   <dbl>\n 1  99.2\n 2  98.9\n 3  98.7\n 4  98.7\n 5  98.9\n 6  98.4\n 7  98.7\n 8  99.6\n 9  99.3\n10  98.7\n# … with 537 more rows\n\npred_BT<- augment(BT_fit, train_data)\n\npred_BT %>% \n  select(BodyTemp, .pred)\n\n# A tibble: 547 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1    100.   99.2\n 2     98.2  98.9\n 3     98.1  98.7\n 4    101.   98.7\n 5     98    98.9\n 6     98    98.4\n 7     98    98.7\n 8    103.   99.6\n 9     99.4  99.3\n10    102.   98.7\n# … with 537 more rows"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-assessment-1-training",
    "href": "fluanalysis/code/modeleval.html#model-assessment-1-training",
    "title": "modeleval",
    "section": "Model Assessment 1 [Training]",
    "text": "Model Assessment 1 [Training]\n\nBT_rmse<- pred_BT %>% \n  rmse(truth=BodyTemp, .pred)\n\nBT_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.12"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-assessment-2-training",
    "href": "fluanalysis/code/modeleval.html#model-assessment-2-training",
    "title": "modeleval",
    "section": "Model Assessment 2 [Training]",
    "text": "Model Assessment 2 [Training]\n\nBT_rsq<- pred_BT %>% \n  rsq(truth=BodyTemp, .pred)\n\nBT_rsq\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.153"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#making-the-recipe-test",
    "href": "fluanalysis/code/modeleval.html#making-the-recipe-test",
    "title": "modeleval",
    "section": "Making the Recipe [Test]",
    "text": "Making the Recipe [Test]\n\nset.seed(321)\n#Creating the recipe \nBT_recipe_test<- recipe(BodyTemp ~., data=test_data)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#workflow-creation-and-prediction-test",
    "href": "fluanalysis/code/modeleval.html#workflow-creation-and-prediction-test",
    "title": "modeleval",
    "section": "Workflow Creation and Prediction [Test]",
    "text": "Workflow Creation and Prediction [Test]\n\n#Now let's set or define our model\nlr_BT_test<- linear_reg() %>%\n  set_engine(\"lm\")\n\n#Creating Workflow\nBT_WF_test<- workflow() %>% \n  add_model (lr_BT_test) %>%\n  add_recipe(BT_recipe)\n\n#Creation of Single Function\nBT_fit_test<- BT_WF_test %>% \n  fit(data= test_data)\n\n#Extracting \nBT_fit_test %>%\n  extract_fit_parsnip() %>%\n  tidy()\n\n# A tibble: 32 × 5\n   term                 estimate std.error statistic   p.value\n   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)           99.0        0.732   135.    1.81e-159\n 2 SwollenLymphNodesYes  -0.0296     0.213    -0.139 8.90e-  1\n 3 ChestCongestionYes    -0.0838     0.225    -0.372 7.10e-  1\n 4 ChillsSweatsYes        0.262      0.275     0.951 3.43e-  1\n 5 NasalCongestionYes    -0.436      0.253    -1.72  8.68e-  2\n 6 SneezeYes              0.0378     0.220     0.172 8.64e-  1\n 7 FatigueYes            -0.0824     0.353    -0.234 8.16e-  1\n 8 SubjectiveFeverYes     0.0292     0.234     0.125 9.01e-  1\n 9 HeadacheYes           -0.217      0.246    -0.880 3.80e-  1\n10 WeaknessMild          -0.377      0.484    -0.779 4.37e-  1\n# … with 22 more rows\n\n#Predicting \npredict(BT_fit_test, test_data)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  99.1\n 2  98.9\n 3  99.9\n 4  98.3\n 5  99.5\n 6  98.8\n 7  99.8\n 8  99.1\n 9  98.9\n10  99.5\n# … with 173 more rows\n\npred_BT_test<- augment(BT_fit_test, test_data)\n\npred_BT_test %>% \n  select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.1\n 2     98.8  98.9\n 3    102.   99.9\n 4     98.2  98.3\n 5     97.8  99.5\n 6     97.8  98.8\n 7    100    99.8\n 8    101.   99.1\n 9     98.8  98.9\n10    100.   99.5\n# … with 173 more rows"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-assessment-1-test",
    "href": "fluanalysis/code/modeleval.html#model-assessment-1-test",
    "title": "modeleval",
    "section": "Model Assessment 1 [Test]",
    "text": "Model Assessment 1 [Test]\n\nBT_rmse_test<- pred_BT_test %>% \n  rmse(truth=BodyTemp, .pred)\n\nBT_rmse_test\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.05"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-assessment-2-test",
    "href": "fluanalysis/code/modeleval.html#model-assessment-2-test",
    "title": "modeleval",
    "section": "Model Assessment 2 [Test]",
    "text": "Model Assessment 2 [Test]\n\nBT_rsq_test<- pred_BT_test %>% \n  rsq(truth=BodyTemp, .pred)\n\nBT_rsq_test\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.147"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#making-the-recipe-body-temperature-runny-nose-training",
    "href": "fluanalysis/code/modeleval.html#making-the-recipe-body-temperature-runny-nose-training",
    "title": "modeleval",
    "section": "Making the Recipe: Body Temperature & Runny Nose [Training]",
    "text": "Making the Recipe: Body Temperature & Runny Nose [Training]\n\nset.seed(321)\n#Creating the recipe \nBTRN_recipe<- recipe(BodyTemp~RunnyNose, data=train_data)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#workflow-creation-and-prediction-training-1",
    "href": "fluanalysis/code/modeleval.html#workflow-creation-and-prediction-training-1",
    "title": "modeleval",
    "section": "Workflow Creation and Prediction [Training]",
    "text": "Workflow Creation and Prediction [Training]\n\n#Now let's set or define our model\nlr_BTRN<- linear_reg() %>%\n  set_engine(\"lm\")\n\n#Creating Workflow\nBTRN_WF<- workflow() %>% \n  add_model (lr_BTRN) %>%\n  add_recipe(BTRN_recipe)\n\n#Creation of Single Function\nBTRN_fit<- BTRN_WF %>% \n  fit(data= train_data)\n\n#Extracting \nBTRN_fit %>%\n  extract_fit_parsnip() %>%\n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.1      0.0964   1028.    0     \n2 RunnyNoseYes   -0.261    0.114      -2.29  0.0225\n\n#Predicting \npredict(BTRN_fit, train_data)\n\n# A tibble: 547 × 1\n   .pred\n   <dbl>\n 1  98.9\n 2  98.9\n 3  98.9\n 4  98.9\n 5  98.9\n 6  98.9\n 7  98.9\n 8  98.9\n 9  99.1\n10  98.9\n# … with 537 more rows\n\npred_BTRN<- augment(BTRN_fit, train_data)\n\npred_BTRN %>% \n  select(BodyTemp, .pred)\n\n# A tibble: 547 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1    100.   98.9\n 2     98.2  98.9\n 3     98.1  98.9\n 4    101.   98.9\n 5     98    98.9\n 6     98    98.9\n 7     98    98.9\n 8    103.   98.9\n 9     99.4  99.1\n10    102.   98.9\n# … with 537 more rows"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-assessment-1-training-1",
    "href": "fluanalysis/code/modeleval.html#model-assessment-1-training-1",
    "title": "modeleval",
    "section": "Model Assessment 1 [Training]",
    "text": "Model Assessment 1 [Training]\n\nBTRN_rmse<- pred_BTRN %>% \n  rmse(truth=BodyTemp, .pred)\n\nBTRN_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.21"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#making-the-recipe-body-temperature-runny-nose-training-1",
    "href": "fluanalysis/code/modeleval.html#making-the-recipe-body-temperature-runny-nose-training-1",
    "title": "modeleval",
    "section": "Making the Recipe: Body Temperature & Runny Nose [Training]",
    "text": "Making the Recipe: Body Temperature & Runny Nose [Training]\n\nset.seed(321)\n#Creating the recipe \nBTRN_recipe_test<- recipe(BodyTemp~RunnyNose, data=test_data)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#workflow-creation-and-prediction-test-1",
    "href": "fluanalysis/code/modeleval.html#workflow-creation-and-prediction-test-1",
    "title": "modeleval",
    "section": "Workflow Creation and Prediction [Test]",
    "text": "Workflow Creation and Prediction [Test]\n\n#Now let's set or define our model\nlr_BTRN_test<- linear_reg() %>%\n  set_engine(\"lm\")\n\n#Creating Workflow\nBTRN_WF_test<- workflow() %>% \n  add_model (lr_BTRN_test) %>%\n  add_recipe(BTRN_recipe)\n\n#Creation of Single Function\nBTRN_fit_test<- BTRN_WF %>% \n  fit(data= test_data)\n\n#Extracting \nBTRN_fit_test %>%\n  extract_fit_parsnip() %>%\n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    99.1       0.154    642.   7.60e-306\n2 RunnyNoseYes   -0.388     0.184     -2.11 3.63e-  2\n\n#Predicting \npredict(BTRN_fit_test, test_data)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  99.1\n 2  98.7\n 3  98.7\n 4  98.7\n 5  99.1\n 6  99.1\n 7  98.7\n 8  99.1\n 9  99.1\n10  99.1\n# … with 173 more rows\n\npred_BTRN_test<- augment(BTRN_fit_test, test_data)\n\npred_BTRN_test %>% \n  select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.1\n 2     98.8  98.7\n 3    102.   98.7\n 4     98.2  98.7\n 5     97.8  99.1\n 6     97.8  99.1\n 7    100    98.7\n 8    101.   99.1\n 9     98.8  99.1\n10    100.   99.1\n# … with 173 more rows"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#model-assessment-1-test-1",
    "href": "fluanalysis/code/modeleval.html#model-assessment-1-test-1",
    "title": "modeleval",
    "section": "Model Assessment 1 [Test]",
    "text": "Model Assessment 1 [Test]\n\nBTRN_rmse_test<- pred_BTRN_test %>% \n  rmse(truth=BodyTemp, .pred)\n\nBTRN_rmse_test\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.13\n\n\nOur training data out preformed our test data, estimates 1.2 and 1.12 respectively. Thus, runny nose does not seem like a predictor of body temperature."
  },
  {
    "objectID": "fluanalysis/data/data_README.html",
    "href": "fluanalysis/data/data_README.html",
    "title": "My Data Analysis Portfolio",
    "section": "",
    "text": "Original data (SympAct_Any_Pos.Rda) is sourced from Brian McKay’s paper titled “Virulence-mediated infectiousness and activity trade-offs and their impact on transmission potential of patients infected with influenza”. The Data set can be found here: https://datadryad.org/stash/dataset/doi:10.5061/dryad.51c59zw4v\nClean data (SypAct_clean.rds) removed columns of non-interest in the wrangling.qmd file. This data set contains categorical variables of flu-symptoms (e.g. vomiting, sore throat) as well as one continuous variable (Body temperature)."
  },
  {
    "objectID": "cheatsheet.html",
    "href": "cheatsheet.html",
    "title": "Cheat Sheet",
    "section": "",
    "text": "Hide source code: {r, echo=FALSE}\nHide text output: {r, results = FALSE} #Summary Tables\nHide messages: {r, message=FALSE} #Package Loading\nHide warning messages: {r, warning=FALSE}\nHide Plots: {r, fig.show = hide}"
  },
  {
    "objectID": "cheatsheet.html#data-cleaning",
    "href": "cheatsheet.html#data-cleaning",
    "title": "Cheat Sheet",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nselect() :which extracts columns from a data frame\nfilter() :which extracts rows from a data frame\n\nFilter NAs: filter(!is.na(x))\n\narrange() :which moves important rows to the top of a data frame\n\nWill naturally put in ascending/alphabetical order.\narrange(desc(x)) will put dataframe in descending order"
  },
  {
    "objectID": "cheatsheet.html#dataset-wrangling",
    "href": "cheatsheet.html#dataset-wrangling",
    "title": "Cheat Sheet",
    "section": "Dataset Wrangling",
    "text": "Dataset Wrangling\n\nCombine column from one dataset to another: - I have column tree in ds1 that I want to add to ds2\n\ncbind command:\ncbind(ds2, x = ds1$tree)\n_join commands\n\nStrsplit: Split up one name into multiple:\n\nExample: BO_19_2938 → BO, 19, 2938 (so 3 columns)\ndata %>% strsplit(split = \"_\")\n\nSubstring: Just take a portion of the name\n\nExample: BO_19_1000\nData %>% substr(1,5) —>Would give us BO_19\n\nFilter columns containing “x”\ndata %>% filter(grepl('x', column))\nRename Column:\nData %>% rename(\"New\" = \"Old\")\nRename Variables In a Column”\nmutate(column = recode(column, \"Old\" = \"Blind New\"))\nMake new column based on old column:\nmutate(new_col = old_col*6) —>If old column had “3”, new column would have “18”\nAverage values based on column (Ex. Triplicates in qPCR)“\ngroup_by(non-numeric things) %>%\nsummarize_if(is.numeric, mean) %>%\nungroup()\nAdd two datasets with same column headings vertically:\nrbind(d1,d2)\nCreate new column using mutate() and case_when:\nDf %>% mutate(new_column = case_when(other_column == \"A\"~ \"What_you_want_in_new_col\")"
  },
  {
    "objectID": "cheatsheet.html#ggplot",
    "href": "cheatsheet.html#ggplot",
    "title": "Cheat Sheet",
    "section": "GGPLOT",
    "text": "GGPLOT\n\nTilting x axis labels:\ntheme(axis.text.x = element_text(angle = 45, hjust = 1))\nCentering Title:\ntheme(plot.title = element_text(hjust = 0.5))\nRemove legend:\ntheme(legend.position = \"none\")\nAdd more ticks to x or y axis:\nscale_x_continuous(n.breaks=10) // scale_y_continuous\nStack Plots with ggarrange():\nggarrange(fig1, fig2, fig3 + font(\"x.text\", size = 10), ncol = 1, nrow = 3)\n\nFinal plot is 1 column and 2 rows\n\nEdit axis or title labels aesthetically:\ntheme(axis.title.y = element_text(hjust = 1, color = \"red, face = \"bold)\nSet colors by factor:\nscale_color_manual(values = c(\"setosa\" = \"purple\",\"versicolor\" = \"red\"))\nSet shapes by factor:\nscale_shape_manual(values = c(\"setosa\" = 1, \"versicolor\" = 2))\n\n\n\nAdd annotation:\nannotate(\"text\", x = 100, y =200, label = \"WORD\", color = \"blue\", fontface=2, size = 6.4)\nAdd Arrows:\nannotate( geom = \"curve\", x = 100, y = 200, xend = 200, yend = 300, curvature = .45, arrow = arrow(length = unit(2, \"mm\")))"
  },
  {
    "objectID": "cheatsheet.html#statistics",
    "href": "cheatsheet.html#statistics",
    "title": "Cheat Sheet",
    "section": "Statistics",
    "text": "Statistics\n\nLinear model: Relationship between infant_mortality (x) and life_expectancy (y) from gapminder dataset:\nlm(infant_mortality\\~life_expectancy, data = gapminder)\nAdd R2 to graph\nbo2 %>% ggplot(aes(x = lag, y = copies_mL)) + geom_point() + stat_smooth(method = \"lm\")+ stat_correlation(method = \"pearson\")+ labs(title = \"LM DustxCopies (BO+1)\", x = \"log(dust)\")"
  },
  {
    "objectID": "cheatsheet.html#exporting",
    "href": "cheatsheet.html#exporting",
    "title": "Cheat Sheet",
    "section": "Exporting",
    "text": "Exporting\n\nSave as RDS file in specific folder:\nsaveRDS(data, file = \"Desktop/cheese/newdata.rds\") would save the file newdata.rds in a folder called cheese on the desktop\nreadRDS(\"Desktop/cheese/newdata.rds\") would load it back in\nSave plot as png:\npng(file = \"file/location/plot.png\")\nGpplot code\ndev.off()"
  },
  {
    "objectID": "cheatsheet.html#random",
    "href": "cheatsheet.html#random",
    "title": "Cheat Sheet",
    "section": "Random",
    "text": "Random\n\nMake a nice table:\ntable<- data.frame(Height, Width)\nkable(table)"
  }
]